---
title: "residency.notes"
author: "Yahya Almodallal"
date: "2024-06-23"
output: html_document
---

```{r setup and libraries, echo=FALSE, include=FALSE}

#Clean environment and free memory
rm(list = ls())
gc()

setwd("C:\\Users\\yahya\\OneDrive\\Research\\residency.notes\\Script")

save_path <- "C:\\Users\\yahya\\OneDrive\\Research\\residency.notes\\Script\\Script cache and figures\\Cache"

knitr::opts_chunk$set(
  echo = FALSE,
  warning = FALSE,
  message = FALSE,
  fig.width = 10,
  fig.height = 8,
  cache = TRUE,
  cache.path = save_path,
  fig.path = save_path
)

#Set number of digits to display
options(digits = 5, scipen = 999)

#Packages and Libraries
library(ggplot2)
library(tidyverse)
library(lubridate)
library(readr)
library(conflicted)
library(dplyr)
library(htmltools)
library(skimr)
library(janitor)
library(gt)
library(tidyr)
library(stringdist)
library(purrr)
library(report)
library(grid)
library(stringr)
library(gridExtra)
library(MASS)
library(knitr)
library(tools)
library(kableExtra)
library(DT)
library(broom)
library(htmlwidgets)
library(webshot)
library(extrafont)
library(ragg)
library(plotly)
library(pagedown)
library(readxl)
library(rlang)
library(nortest)
library(glue)
library(reshape2)
library(GGally)
library(boot)
library(car)
library(ggpubr)
library(DescTools)
library(multcomp)
library(rstatix)
library(broom)
library(patchwork)
conflict_prefer("select", "dplyr")
conflicts_prefer(dplyr::filter)  # Set conflict resolution preferences



```


```{r, functions, echo=FALSE, include=FALSE}

#summarize dataframes
sum_df <- function(df) {
  # Create an empty list to store summaries
  summary_list <- list()
  
  # 1. Number of rows
  summary_list$Number_of_Rows <- nrow(df)
  
  # 2. Names of columns
  summary_list$Column_Names <- colnames(df)
  
  # 3. Summaries for character variables with fewer than 100 unique values
  char_vars <- df[, sapply(df, is.character)]
  if (!is.null(char_vars)) {
    summary_list$Character_Variables <- lapply(char_vars, function(x) {
      if (length(unique(x)) < 100) {
        return(table(x))
      } else {
        return(NA)  # Or return(NULL) if you prefer not to store anything for columns with >= 100 unique values
      }
    })
  }
  
  
  # 4. Summaries for numerical variables
  num_vars <- df[, sapply(df, is.numeric)]
  if (!is.null(num_vars)) {
    summary_list$Numerical_Variables <- lapply(num_vars, function(x) {
      list(
        Median = median(x, na.rm = TRUE) / 60,
        IQR_min = IQR(x, na.rm = TRUE) / 60,
        Min_min = min(x, na.rm = TRUE) / 60,
        Max_min = max(x, na.rm = TRUE) / 60,
        log_avg_min = exp(mean(log(x / 60), na.rm = TRUE)),
        log_sd_min = exp(sd(log(x / 60), na.rm = TRUE)),
        total_day = sum(x, na.rm = TRUE) / (60 * 60 * 24)
      )
    })
  }
  
  # 5. First and last dates for POSIXct variables
  date_vars <- df[, sapply(df, function(x) inherits(x, "POSIXct"))]
  if (!is.null(date_vars)) {
    summary_list$Date_Variables <- lapply(date_vars, function(x) {
      list(
        First_Date = min(x, na.rm = TRUE),
        Last_Date = max(x, na.rm = TRUE)
      )
    })
  }
  
  return(summary_list)
}


# Determine the 'Shift'
get_shift <- function(encounter, editstart) {
  hour <- hour(editstart)
  if (encounter %in% c("Ambulatory", "HOV", "Telephone Encounter")) {
    if (hour >= 7 & hour < 17) {
      return("Work-hours")
    } else {
      return("After work-hours")
    }
  } else if (encounter == "Emergency Department") {
    return(NA)
  } else if (encounter %in% "Inpatient") {
    if (hour >= 7 & hour < 18) {
      return("AM shift")
    } else {
      return("PM shift")
    }
  } else {
    return(NA)
  }
}

# Function to create and save plots for character variables in EDA
create_and_save_plots <- function(data, columns, file_name_prefix, ncol = 3) {
  plots <- lapply(columns, function(column) {
    ggplot(data, aes(x = !!sym(column))) + 
      geom_bar() +
      geom_text(stat='count', aes(label=..count..), vjust=-0.5) +
      theme(axis.text.x = element_text(angle = 90, hjust = 1))
  })
  
  combined_plot <- do.call(grid.arrange, c(plots, ncol = ncol))
  
  # Save the combined figure
  ggsave(paste0(file_name_prefix, "_combined_plot.png"), plot = combined_plot, width = 20, height = 15)
}

# Function to create and save plots for numeric variables in EDA
create_and_save_numeric_plots <- function(data, file_name_prefix, ncol = 3) {
  numeric_cols <- data %>%
    select_if(is.numeric) %>%
    colnames()
  
  plots <- lapply(numeric_cols, function(column) {
    hist_plot <- ggplot(data, aes(x = !!sym(column))) + 
      geom_histogram(bins = 60) +
      labs(title = column)
    
    list(hist_plot)
  })
  
  plots <- unlist(plots, recursive = FALSE)
  
  # Split plots into groups of 6 and save each group
  plot_groups <- split(plots, ceiling(seq_along(plots)/6))
  
  for (i in seq_along(plot_groups)) {
    combined_plot <- do.call(grid.arrange, c(plot_groups[[i]], ncol = ncol))
    ggsave(paste0(file_name_prefix, "_combined_plot", i, ".png"), plot = combined_plot, width = 20, height = 15)
  }
}


# Function to apply transformations, sample means, create plots, and perform tests
analyze_transformations <- function(data, columns, seed = 12) {
  test_results <- data.frame(
    variable = character(),
    transformation = character(),
    ad_p_value = numeric(),
    shapiro_p_value = numeric(),
    stringsAsFactors = FALSE
  )
  
  transformations <- list(
    original = function(x) x,
    log = function(x) ifelse(x > 0, log(x), NA),
    sqrt = function(x) ifelse(x >= 0, sqrt(x), NA),
    cbrt = function(x) ifelse(x >= 0, x^(1/3), NA),
    boxcox = function(x) {
      x_positive <- x + 1
      lambda <- boxcox(x_positive ~ 1, lambda = seq(-2, 2, by = 0.05))$x[which.max(boxcox(x_positive ~ 1, lambda = seq(-2, 2, by = 0.05))$y)]
      (x_positive)^lambda
    }
  )
  
  for (column in columns) {
    for (trans_name in names(transformations)) {
      # Apply transformation
      trans_data <- data %>% mutate(trans_col = transformations[[trans_name]](!!sym(column)))
      
      # Remove NA values from the transformed data
      clean_data <- na.omit(trans_data$trans_col)
      
      if (length(clean_data) >= 500) {  # Ensure there are enough data points for sampling
        set.seed(seed)  # for reproducibility
        sample_means <- replicate(1000, mean(sample(clean_data, size = 500, replace = TRUE)))
        
        # Create histogram
        hist(sample_means, breaks = 60, main = paste("Histogram of Sample Means -", column, trans_name))
        
        # Create QQ plot
        qqnorm(sample_means, main = paste("QQ Plot of Sample Means -", column, trans_name))
        qqline(sample_means, col = "red")
        
        # Perform tests
        ad_test <- ad.test(sample_means)
        shapiro_test <- shapiro.test(sample_means)
        
        # Store results
        test_results <- rbind(test_results, data.frame(
          variable = column,
          transformation = trans_name,
          ad_p_value = ad_test$p.value,
          shapiro_p_value = shapiro_test$p.value
        ))
      } else {
        message(paste("Not enough valid data points for", column, "with transformation", trans_name))
      }
    }
  }
  
  return(test_results)
}

# Function to summarize times for each resident in the list of dataframes where each dataframe is the notes of a resident
calculate_statistics <- function(df) {
  all_notes <- nrow(df)
  inpatient_notes <- sum(df$encountercontext == "Inpatient")
  ambulatory_notes <- sum(df$encountercontext == "Ambulatory")
  ed_notes <- sum(df$encountercontext == "Emergency Department")
  telephone_notes <- sum(df$encountercontext == "Telephone Encounter")
  total_time_days <- sum(df$resident.time) / (60 * 60 * 24)
  mean_time_mins <- exp(mean(log(df$resident.time / 60), na.rm = TRUE))
  sd_time_mins <- exp(sd(log(df$resident.time / 60), na.rm = TRUE))
  median_time_mins <- median(df$resident.time / 60)
  max_time_mins <- max(df$resident.time / 60)
  quantile_25th_time_mins <- quantile(df$resident.time / 60, probs = 0.25)
  quantile_75th_time_mins <- quantile(df$resident.time / 60, probs = 0.75)
  weekend_time_days <- sum(df$weekend) / (60 * 60 * 24)
  percentage_weekend <- sum(df$weekend) / sum(df$resident.time) * 100
  pm_time_days <- sum(df$pm, na.rm = TRUE) / (60 * 60 * 24)
  percentage_pm <- sum(df$pm, na.rm = TRUE) / sum(df$resident.time) * 100
  after_hours_time_days <- sum(df$after.hours, na.rm = TRUE) / (60 * 60 * 24)
  percentage_after_hours <- sum(df$after.hours, na.rm = TRUE) / sum(df$resident.time) * 100
  mean_note_size <- exp(mean(log(df$notesize), na.rm = TRUE))
  sd_note_size <- exp(sd(log(df$notesize), na.rm = TRUE))
  count_dictated <- sum(df$`%voice` > 0, na.rm = TRUE)
  mean_dictation <- mean(df$`%voice`, na.rm = TRUE)
  sd_dictation <- sd(df$`%voice`, na.rm = TRUE)
  count_copied <- sum(df$`%copied` > 0, na.rm = TRUE)
  mean_copied <- mean(df$`%copied`, na.rm = TRUE)
  sd_copied <- sd(df$`%copied`, na.rm = TRUE)

  # Specific mean times calculations with exponential and log transformation
  mean_hp <- ifelse(any(df$notetype == "Admission Note"), exp(mean(log(df$resident.time[df$notetype == "Admission Note"] / 60), na.rm = TRUE)), NA)
  mean_CN <- ifelse(any(df$notetype == "Clinic Note"), exp(mean(log(df$resident.time[df$notetype == "Clinic Note"] / 60), na.rm = TRUE)), NA)
  mean_DS <- ifelse(any(df$notetype == "Discharge Summary"), exp(mean(log(df$resident.time[df$notetype == "Discharge Summary"] / 60), na.rm = TRUE)), NA)
  mean_Cs <- ifelse(any(df$notetype == "Consults"), exp(mean(log(df$resident.time[df$notetype == "Consults"] / 60), na.rm = TRUE)), NA)
  mean_PN <- ifelse(any(df$notetype == "Progress Notes"), exp(mean(log(df$resident.time[df$notetype == "Progress Notes"] / 60), na.rm = TRUE)), NA)
  mean_TE <- ifelse(any(df$notetype == "Telephone Encounter"), exp(mean(log(df$resident.time[df$notetype == "Telephone Encounter"] / 60), na.rm = TRUE)), NA)
  mean_ED <- ifelse(any(df$notetype == "Emergency Department"), exp(mean(log(df$resident.time[df$notetype == "Emergency Department"] / 60), na.rm = TRUE)), NA)
  mean_P <- ifelse(any(df$notetype == "Procedures"), exp(mean(log(df$resident.time[df$notetype == "Procedures"] / 60), na.rm = TRUE)), NA)

  smartphrase_mean <- mean(lengths(gregexpr("\\[", df$smartphraseids)), na.rm = TRUE)
  time_to_complete_mean <- exp(mean(log(df$starttostop), na.rm = TRUE)) / 60 * 24
  time_to_complete_sd <- exp(sd(log(df$starttostop), na.rm = TRUE))
  number_copied_forward <- sum(df$iteration > 0, na.rm = TRUE)
  mean_iteration <- mean(df$iteration, na.rm = TRUE)
  sd_iteration <- sd(df$iteration, na.rm = TRUE)

  # Create a dataframe of the computed values
  data.frame(
    'All.Notes' = all_notes,
    'Inpatient' = inpatient_notes,
    'Ambulatory' = ambulatory_notes,
    EDNotes = ed_notes,
    TelephoneNotes = telephone_notes,
    TotalTimeDays = total_time_days,
    Overall = mean_time_mins,
    SDTimeMins = sd_time_mins,
    MedianTimeMins = median_time_mins,
    MaxTimeMins = max_time_mins,
    IQR25th = quantile_25th_time_mins,
    IQR75th = quantile_75th_time_mins,
    WeekendTimeDays = weekend_time_days,
    Weekend = percentage_weekend,
    PMTimeDays = pm_time_days,
    'Inpatient.PM' = percentage_pm,
    AfterHoursTimeDays = after_hours_time_days,
    'Ambulatory.After_Hours' = percentage_after_hours,
    'Note.Size' = mean_note_size,
    SDNoteSize = sd_note_size,
    CountDictated = count_dictated,
    'Percent.Dictated' = mean_dictation,
    SDDictation = sd_dictation,
    CountCopied = count_copied,
    'Percent.Copied' = mean_copied,
    SDCopied = sd_copied,
    SmartphraseMean = smartphrase_mean,
    'Days.to.Sign' = time_to_complete_mean,
    TimeToCompleteSD = time_to_complete_sd,
    NumberCopiedForward = number_copied_forward,
    MeanIteration = mean_iteration,
    SDIteration = sd_iteration,
    'HP' = mean_hp,
    'Clinic' = mean_CN,
    'Discharge' = mean_DS, 
    'Consult' = mean_Cs, 
    'Progress' = mean_PN, 
    'Telephone' = mean_TE, 
    'ED' = mean_ED, 
    'Procedure' = mean_P
  )
}

# Function to melt and create plots for specific variable groups without legend
create_melted_plot <- function(data, columns, title, x_label, y_label) {
  melted_data <- melt(data[, c("PGY", columns)], id.vars = "PGY")
  ggplot(melted_data, aes(x = variable, y = value, color = PGY)) +
    geom_point(position = position_jitter(width = 0.2, height = 0)) +
    labs(title = title, x = x_label, y = y_label) +
    theme_minimal() +
    theme(axis.text.x = element_text(angle = 45, hjust = 1),
          legend.position = "none")
}



perform_analysis <- function(df, cont_var, cat_var) {
  # Ensure there are no zeros or negative values in the continuous variable
  df <- df %>%
  select(!!sym(cat_var), !!sym(cont_var)) %>%
  mutate(!!sym(cont_var) := ifelse(!!sym(cont_var) <= 0, NA, !!sym(cont_var))) %>%
  filter(!is.na(!!sym(cont_var))) %>%
  mutate(log_transformed = log(!!sym(cont_var)))

  # Descriptive statistics on log-transformed data
  descriptive_stats <- df %>%
    group_by(!!sym(cat_var)) %>%
    summarise(
      Count_Non_NA = n(),
      Real_Mean = exp(mean(log_transformed, na.rm = TRUE)),
      sd = exp(sd(log_transformed, na.rm = TRUE)),
      N = sum(!is.na(log_transformed)),
      sd_log = sd(log_transformed, na.rm = TRUE),
      SE = exp(sd_log) / sqrt(N),
      ci_lower = exp(mean(log_transformed, na.rm = TRUE) - qt(0.975, df = n() - 1) * sd_log / sqrt(N)),
      ci_upper = exp(mean(log_transformed, na.rm = TRUE) + qt(0.975, df = n() - 1) * sd_log / sqrt(N)),
      .groups = 'drop'
    )

  # Filter data for groups that have at least 10 observations
  df <- df %>%
    group_by(!!sym(cat_var)) %>%
    filter(n() >= 10) %>%
    ungroup()

  # Create formula for ANOVA
  anova_formula <- as.formula(paste("log_transformed ~", cat_var))

  # Welch's ANOVA on log-transformed data
  if (nrow(df) > 10) {
    anova_results <- df %>%
      welch_anova_test(anova_formula)
    } 

  # Create formula for Games-Howell test
  posthoc_formula <- as.formula(paste("log_transformed ~", cat_var))

  # Games-Howell post-hoc test on log-transformed data
  post_hoc_results <- df %>%
    games_howell_test(posthoc_formula)

  
  # Reversing groups and adjusting corresponding values
  reversed_post_hoc_results <- post_hoc_results %>%
  mutate(
    # Swap groups
    old_group1 = group1,
    group1 = group2,
    group2 = old_group1,
    
    # Reverse the signs of estimates and confidence intervals
    estimate = -estimate,
    old_conf_low = conf.low,
    conf.low = -conf.high,
    conf.high = -old_conf_low
  ) %>%
  select(-old_group1, -old_conf_low)  # Removing temporary columns

  # Combine the original and reversed data frames
  combined_results <- bind_rows(post_hoc_results, reversed_post_hoc_results) %>%
  distinct

  # Merge to add Real Means
  combined_results <- combined_results %>%
    left_join(descriptive_stats %>% select(!!sym(cat_var), N1 = N, LCI1 = ci_lower, UCI1 = ci_upper, sd1 = sd, se1 = SE, Real_Mean1 = Real_Mean), by = c("group1" = cat_var)) %>%
    left_join(descriptive_stats %>% select(!!sym(cat_var), N2 = N, LCI2 = ci_lower, UCI2 = ci_upper, sd2 = sd, se2 = SE, Real_Mean2 = Real_Mean), by = c("group2" = cat_var)) %>%
    mutate(
      Mean_Difference = Real_Mean2 - Real_Mean1,
      Estimated_Real_Mean2 = Real_Mean1 * exp(estimate),
      CI_Real_Mean2_Lower = Real_Mean1 * exp(conf.low),
      CI_Real_Mean2_Upper = Real_Mean1 * exp(conf.high)
    )

  return(combined_results)
}


```


```{r upload & combine, eval=FALSE, echo=FALSE, include=FALSE}

# Specify file paths
raw_files <- c(
  "C:\\Users\\yahya\\OneDrive\\Research\\residency.notes\\Raw files\\AllNotes124624_20240707_1002.xlsx",
  "C:\\Users\\yahya\\OneDrive\\Research\\residency.notes\\Raw files\\AllNotes721622_20240623_1717.xlsx",
  "C:\\Users\\yahya\\OneDrive\\Research\\residency.notes\\Raw files\\AllNotes722623_20240623_1715.xlsx",
  "C:\\Users\\yahya\\OneDrive\\Research\\residency.notes\\Raw files\\AllNotes7231223_20240623_1719.xlsx"
)

# Read each file and combine rows
all_data_1 <- suppressWarnings(
  lapply(raw_files, read_excel) %>%
    bind_rows()
)

# Names of all residents over the last 3 years, per year. Names have changed to some extent overtime, the names in the above uploaded files will be adjusted to match the ones in this file
resident.names <- read.csv("C:\\Users\\yahya\\OneDrive\\Research\\residency.notes\\Edited files\\All Residents 21-24.csv")

# Or if you want to convert them to a vector
res_vector <- unlist(resident.names)

# The true service of all attendings, inspired by earlier work and updated in 7/2024
true.service <- read_xlsx("C:\\Users\\yahya\\OneDrive\\Research\\residency.notes\\Edited files\\true.author.service.7.24.xlsx")

# Filter rows where 'authortype' is 'attending'
staffs <- true.service %>%
  filter(authortype == "attending") %>%
  pull(name)

```


```{r refine, eval=FALSE, echo=FALSE, include=FALSE}

# Convert all column names to lowercase and remove spaces
names(all_data_1) <- tolower(gsub("[ '?]", "", names(all_data_1)))

# Refining: rename columns, column class, nnsy service, delete columns
all_data_1 <- all_data_1 %>%
  rename(
    specialtyatnote = `specialty...19`,
    specialtycurrent = `specialty...20`
  ) %>%
  mutate(
    # Edit column class, remove % symbol
    `%voice` = as.numeric(gsub("%", "", `%voice`)),
    `%copied` = as.numeric(gsub("%", "", `%copied`)),
    noteid = as.character(noteid),
    # Recategorize 'authortype'
    authortype = case_when(
      authortype %in% c("(Other)", "Clerk", "Dietitian", "Medical Student", "Non-Privileged Provider", "Nurse-ARNP", "Nursing Assistant", "Pharmacist", "Physician-Fellow", "Dentist-Resident", "Medical Assistant", "Occupational Therapist", "PA Student", "Physical Therapist", "Speech Pathologist", "Student Nurse", "Physician Assistant", "Registered Nurse", "Social Worker") ~ "other",
      authortype %in% c("Anesthesiologist", "Physician-Associate", "Physician-Fellow Associate", "Physician-Staff", "Physician-Visit Assistant Prof", "Physician-Visit Associate") ~ "attending",
      authortype == "Physician-Resident" ~ "resident",
      TRUE ~ authortype # Preserve original for undefined categories
    )
  ) %>%
    # Set encountercontext for specific notetypes
  mutate(encountercontext = case_when(
    notetype %in% c("Clinic Note", "Clinical Letter") ~ "Ambulatory",
    notetype %in% c("Discharge Summary", "Admission Note", "Interim Progress Note", "Progress Notes") ~ "Inpatient",
    notetype %in% c("ED Provider Notes", "ED Notes") ~ "Emergency Department",
    notetype %in% c("Telephone Encounter") ~ "Telephone Encounter",
    TRUE ~ encountercontext
  )) %>%
  mutate(authorsservice = case_when(
      is.na(authorsservice) | authorsservice == "" ~ "unknown",  # Checks for NA or empty strings and replaces with "unknown"
      authorsservice %in% c("Pediatrics", "QuickCare") ~ "PED General",
      authorsservice == "Psychiatry" ~ "PSY Child Psychiatry",
      authorsservice == "MED Allergy/Immunology" ~ "PED Allergy/Immunology",
      TRUE ~ authorsservice
  )) %>%
  mutate(notetype = case_when(
    notetype %in% c("ED Provider Notes", "ED Notes") ~ "Emergency Department",
    TRUE ~ notetype
  )) %>%
  # Change encountercontext values of "HOV"
  mutate(encountercontext = if_else(encountercontext == "HOV", "Ambulatory", encountercontext)) %>%
  mutate(notetype = if_else(notetype == "H&P", "Admission Note", notetype)) %>%
  # Set authorsservice for Emergency Department
  mutate(authorsservice = if_else(encountercontext == "Emergency Department", "Emergency Department", authorsservice)) %>%
  mutate(encountercontext = if_else(authorsservice == "Emergency Department", "Emergency Department", encountercontext)) %>%
  mutate(notetype = if_else(encountercontext == "Emergency Department", "Emergency Department", notetype)) %>%
  mutate(notetype = if_else(encountercontext == "Telephone Encounter", "Telephone Encounter", notetype)) %>%
  # Update encountercontext for specific services
  mutate(encountercontext = case_when(
    (encountercontext %in% c("Ambulatory", "Inpatient") & authorsservice == "Emergency Department") ~ "Emergency Department",
    TRUE ~ encountercontext
  ))%>%
  # Remove unneeded columns
  select(-noteactive, -`%***`)

# summary.1: raw data after some refining
summary.1 <- sum_df(all_data_1)

# Identifying notes with equal number of authors and edit times, excluding notes with unequal numbers, and identifying notes with no time data
  # available and equal edit times and author information
  all_data_2 <- all_data_1 %>%
    mutate(
      authorcount = str_count(author, "\n") + 1,
      timecount = str_count(`edittime(seconds)`, "\n") + 1,
    ) %>%
    filter(authorcount == timecount) %>%
    select(-authorcount, -timecount)

  # not equal number (too many edit instances)
  not_equal <- all_data_1 %>%
    mutate(
      authorcount = str_count(author, "\n") + 1,
      timecount = str_count(`edittime(seconds)`, "\n") + 1
    ) %>%
    filter(authorcount != timecount) %>%
    select(-authorcount, -timecount)

  # not available edit time information
  combined <- bind_rows(not_equal, all_data_2)
    no_times <- anti_join(all_data_1, combined, by = "noteid")

  all_exported_count = nrow(all_data_1)
  after_deleteing_incomplete_data_count = nrow(all_data_2)
  inequal_number_count = nrow(not_equal)
  no_time_data_count = nrow(no_times)
  
  print(nrow(all_data_1))
  print(nrow(all_data_2))
  print(nrow(not_equal))
  print(nrow(no_times))
  
  
# The edit start and edit stop dates are not uniform, there were about 3000 values where the dates are shown as serial numbers, luckily these are easy to extract since each has a single editing instance
  # Extract serial number dates and convert them back to actual dates, then calculate the time between the start and end of each note
  filtered_data <- all_data_2 %>%
    filter(grepl("^[0-9]+\\.?[0-9]*$", editstart), grepl("^[0-9]+\\.?[0-9]*$", editstop)) %>%
    mutate(
      editstart = as.POSIXct(as.Date(as.numeric(editstart), origin = "1899-12-30"), tz = "UTC"),
      editstop = as.POSIXct(as.Date(as.numeric(editstop), origin = "1899-12-30"), tz = "UTC"),
      starttostop = difftime(editstop, editstart, units = "hours"),
      `edittime(seconds)` = as.numeric(`edittime(seconds)`)
    )
  
  # Remaining rows, to calculate the time between the start and end of these notes; will need to divide the contents of each row, convert them to date/time, then calculate the difference
    # Step 1: Filter out rows in all_data_2 that match rows in filtered_data
    all_data_2_remaining <- all_data_2 %>%
      anti_join(filtered_data, by = "noteid") %>%
      mutate(
      first_editstart = sapply(strsplit(as.character(editstart), "\r\n"), `[`, 1),
      last_editstop = sapply(strsplit(as.character(editstop), "\r\n"), tail, 1),
      first_editstart = as.POSIXct(first_editstart, format = "%m/%d/%Y %I:%M:%S %p", tz = "UTC"),
      last_editstop = as.POSIXct(last_editstop, format = "%m/%d/%Y %I:%M:%S %p", tz = "UTC"),
      starttostop = difftime(last_editstop, first_editstart, units = "hours")
    ) %>%
      select(-last_editstop, -first_editstart)
  
# Separate the editing instances into separate rows
  all_data_3 <- all_data_2_remaining %>% separate_rows(editstart, editstop, author, 'edittime(seconds)', sep = "\r\n")
    # Convert dates to proper format, then deal with NA values
      all_data_3$dtofsvc <- as.POSIXct(all_data_3$dtofsvc, format="%m/%d/%Y %I:%M:%S %p")
      all_data_3$filetime <- as.POSIXct(all_data_3$filetime, format="%m/%d/%Y %I:%M:%S %p")
      all_data_3$editstart <- as.POSIXct(all_data_3$editstart, format="%m/%d/%Y %I:%M:%S %p")
      all_data_3$editstop <- as.POSIXct(all_data_3$editstop, format="%m/%d/%Y %I:%M:%S %p")
      all_data_3$`edittime(seconds)` <- as.numeric(all_data_3$`edittime(seconds)`)

  # Now we can join the two dataframes, and double checks for NAs, zeros, or negative values
  all_data_3 <- bind_rows(all_data_3, filtered_data)
  
  # Replace 0 or negative values with NA
  all_data_3$starttostop <- ifelse(all_data_3$starttostop <= 0, NA, all_data_3$starttostop)
  
  # Check for NA values
  na_count = sum(is.na(all_data_3$starttostop))
  total_rows = nrow(all_data_3)
  
  # Print the results
  print(paste("Number of NA values: ", na_count))
      
# Adding the day and shift columns
  all_data_3 <- all_data_3 %>%
   mutate(day = weekdays(editstart)) %>%
   mutate(shift = mapply(get_shift, encountercontext, editstart))

    # Calculate the number of NAs in editstart and shift columns
    editstart_na_count <- sum(is.na(all_data_3$editstart))
    shift_na_count <- sum(is.na(all_data_3$shift))
    
    # Count how many notes are from the Emergency Department
    ed_notes_count <- sum(all_data_3$encountercontext == "Emergency Department")
    
    # Check that all NAs are for ED notes and the count of NAs matches the count of ED notes
    if (editstart_na_count == 0 && shift_na_count == ed_notes_count) {
        print("All dates are appropriate and only ED notes have no assigned shift")
    } else {
        print("There is a discrepancy in the data. Not all NAs are for ED notes or the counts do not match.")
    }


# Adding weekend, after hours (for ambulatory notes), and PM shift (for inpatient notes) columns
all_data_3 <- all_data_3 %>%
  mutate(
    weekend = if_else(day %in% c("Saturday", "Sunday"), `edittime(seconds)`, NA_real_),
    after.hours = if_else(shift == "After work-hours", `edittime(seconds)`, NA_real_),
    pm = if_else(shift == "PM shift", `edittime(seconds)`, NA_real_)
  )
    
# Cleaning the author and note author columns and making sure every signing note author name is present as an author of the same note
  
  # Clean the Author column (remove the [########] from the end of each name)
    all_data_3 <- all_data_3 %>%
      mutate(
        author = gsub("\\s\\[\\d+\\]", "", author)
      )

  # Convert author and note author columns to lower case
    all_data_3 <- all_data_3 %>%
      mutate(
        noteauthor = tolower(noteauthor),
        author = tolower(author)
      )

    # Remove spaces at the end of each value, and remove everything after the 2nd comma (individual's degree)
      all_data_4 <- all_data_3 %>%
        mutate(noteauthor = sapply(str_split(noteauthor, ", "), function(x) paste(x[1:min(length(x), 2)], collapse = ", ")),
               noteauthor = if_else(str_sub(noteauthor, -1, -1) == " ", str_sub(noteauthor, 1, str_length(noteauthor) - 1), noteauthor),
               author = if_else(str_sub(author, -1, -1) == " ", str_sub(author, 1, str_length(author) - 1), author))

    # Filter all rows where a unique note's signing author is not present in the authors' column
      filtered_data <- all_data_4 %>%
        group_by(noteid) %>%
        filter(!noteauthor %in% author) %>%
        ungroup()

    # Take these rows out of the main dataframe, clean them, then join them back into the main dataframe
      unfiltered <- all_data_4 %>%
        anti_join(filtered_data, by = colnames(all_data_4))
      
      nrow(filtered_data) + nrow(unfiltered) == nrow(all_data_4)

      # Define the mapping of original values to new values
        author_mapping <- c(
          "bea..." = "beasley, gary s",
          "colaizy, ..." = "colaizy, tarah t",
          "dagle, jo..." = "dagle, john m",
          "fontinel, amy l" = "stier, amy c",
          "zhorne, derek j" = "weinstein, stuart l"
        )
        
        # Apply the mapping to the 'author' column
          filtered_data <- filtered_data %>%
            mutate(author = case_when(
              author %in% names(author_mapping) ~ author_mapping[author],
              TRUE ~ author
            ))
        
        # Define the mapping of original values to new values for the noteauthor column
          noteauthor_mapping <- c(
            "abu hamad, moh" = "abu hamad, moh'd rawhi abdullah m r",
            "abu hamad, moh'd rawhi a" = "abu hamad, moh'd rawhi abdullah m r",
            "aguilar, agustin jr." = "aguilar, agustin",
            "brown, adam l" = "brown, ashley",
            "cornelius, kacie" = "rytlewski, kacie",
            "daniels, elizabeth c" = "smet, elizabeth c",
            "ebach, dawn" = "ebach, dawn r",
            "haskell, sarah e" = "haskell, sarah",
            "lee, samantha b" = "lee, samantha",
            "lin-dyken, deborah" = "lin-dyken, deborah c",
            "ma, melinda a" = "whitacre, melinda a",
            "madthil, sujana" = "madathil, sujana",
            "meyer, heather" = "schmuecker, heather m",
            "newton, kristina" = "sobaski, kristina t",
            "norris, andrew" = "norris, andrew w",
            "ramsey, laura" = "ramsey, laura j",
            "reasoner, andrea l" = "porter, andrea l",
            "scheffler, stephanie m" = "james, stephanie s",
            "smith-mccartney, lindsey m" = "ewan, lindsey m",
            "springman, alexandra l" = "keating, alexandra l"
          )
        
        # Apply the mapping to the 'noteauthor' column
          filtered_data <- filtered_data %>%
            mutate(noteauthor = case_when(
              noteauthor %in% names(noteauthor_mapping) ~ noteauthor_mapping[noteauthor],
              noteauthor == "botos, madison" ~ "limke, wyatt l",
              TRUE ~ noteauthor
            ))

        # Now join the fixed filtered data and the unfiltered data
        all_data_5 <- bind_rows(filtered_data, unfiltered)
        nrow(all_data_5) == nrow(all_data_4)
        
        # Check: Filter all rows where a unique note's signing author is not present in the authors' column; make sure rows are 0 now
          any_issues <- all_data_5 %>%
            group_by(noteid) %>%
            filter(!noteauthor %in% author) %>%
            ungroup()
          nrow(any_issues) == 0

# Cleaning resident names, adjust specific residents with changed names to match those in 'resident.names'

  # Create a mapping vector (those names are inspired from previous work in 2022, however, from exploring the current data, it seems that the only different name is that of springman, alexandra which is now keating, alexandra)
    name_mapping <- c(
      "abu hamad, moh" = "abu hamad, moh'd rawhi abdullah m r",
      "mcintire, natalie" = "mcintire, natalie r",
      "daniels, elizabeth c" = "smet, elizabeth c",
      "stover, daniel" = "stover, daniel w",
      "ma, melinda a" = "whitacre, melinda a",
      "keating, alexandra l" = "springman, alexandra l",
      "cornelius, kacie" = "rytlewski, kacie",
      "rytlewski, kacie t" = "rytlewski, kacie"
    )
  
  # Apply the mapping to the 'noteauthor' and 'author' columns
    all_data_6 <- all_data_5 %>%
      mutate(author = case_when(
        author %in% names(name_mapping) ~ name_mapping[author],
        TRUE ~ author
      )) %>%
      mutate(noteauthor = case_when(
        noteauthor %in% names(name_mapping) ~ name_mapping[noteauthor],
        TRUE ~ noteauthor
      ))
    
# Cleaning author service and author type (this is based on work done in 9/2023 and extended here to include any new providers; the unique noteauthors were exported, and the author service, and type, adjusted manually to crate the true.author.service file)
    
    # Join the file with the manually adjusted data to the main file
    all_data_7 <- all_data_6 %>%
      left_join(true.service, by = c("noteauthor" = "name")) %>%
      # Update authorsservice
      mutate(authorsservice = coalesce(authorsservice.y, authorsservice.x)) %>%
      # Update authortype
      mutate(authortype = coalesce(authortype.y, authortype.x)) %>%
      # Remove the temporary .x and .y columns for both fields
      select(-matches("\\.x$"), -matches("\\.y$")) %>%
      # Update the authorservice to PED General if a reisdent wrote the note and the author service is NA
      mutate(authorsservice = if_else(is.na(authorsservice) & authortype == "resident", "PED General", authorsservice))

# Check that all "attendings" are present in true.service

  # Step 1: Filter attending authors in all_data_7
  attending_authors <- all_data_7 %>%
    filter(authortype == "attending") %>%
    select(noteauthor) %>%
    distinct()
  
  # Step 2: Check against true.service names
  unmatched_authors <- anti_join(attending_authors, true.service, by = c("noteauthor" = "name"))
  
  # Step 3: Provide a warning if there are discrepancies
  if(nrow(unmatched_authors) > 0) {
    warning("Fix the author service for any new note authors")
    print(unmatched_authors) # Optionally print out the unmatched authors for review
  } else {
    print("All attending note authors are correctly matched in true.service")
  }
  
  # Step 4: Provide a warning if there are discrepancies
  if(nrow(all_data_3) != nrow(all_data_7)) {
    warning("You have lost some rows while refining the data")
    print(unmatched_authors) # Optionally print out the unmatched authors for review
  } else {
    print("You have not lost any rows while refining the data")
  }

  # Fixing the PED Nursery author service
  all_data_8 <- all_data_7 %>%
    mutate(
      # Create a new column 'NNSY' based on specified conditions
      nnsy = ifelse(
        abs(difftime(dtofsvc, birthdate, units = "days")) <= 3 &
          authorsservice %in% c("PED General", "PED Nursery", "Pediatrics") &
          encountercontext %in% c("Inpatient") &
          notetype != "Clinic Note",
        "Yes",
        "No"
      )
    ) %>%
    # Update 'Author.s.Service' for 'NNSY' = "Yes"
    mutate(
      authorsservice = ifelse(nnsy == "Yes", "PED Nursery", authorsservice),
    ) %>%
    # Remove unneeded columns
    select(-nnsy)
  
summary.6 = sum_df(all_data_8)

# Determine PGY for each resident and for each time interval
  # Define the date ranges
  date_21_start <- as.POSIXct("2021-07-01 00:00:00")
  date_21_end <- as.POSIXct("2022-06-30 23:59:59")
  date_22_start <- as.POSIXct("2022-07-01 00:00:00")
  date_22_end <- as.POSIXct("2023-06-30 23:59:59")
  date_23_start <- as.POSIXct("2023-07-01 00:00:00")
  date_23_end <- as.POSIXct("2024-06-30 23:59:59")
  before_end <- as.POSIXct("2021-06-30 23:59:59")
  after_start <- as.POSIXct("2024-07-01 00:00:00")
  
  # Filter the data based on the date ranges
  data_21 <- all_data_8 %>% filter(editstart >= date_21_start & editstart <= date_21_end)
  data_22 <- all_data_8 %>% filter(editstart >= date_22_start & editstart <= date_22_end)
  data_23 <- all_data_8 %>% filter(editstart >= date_23_start & editstart <= date_23_end)
  before <- all_data_8 %>% filter(editstart <= before_end)
  after <- all_data_8 %>% filter(editstart >= after_start)

  # Check that no values were missed
  total_rows <- nrow(all_data_8)
  filtered_rows <- nrow(data_21) + nrow(data_22) + nrow(data_23) + nrow(before) + nrow(after)
  
  if(total_rows == filtered_rows) {
    print("All rows have been accounted for.")
  } else {
    print("Some rows have been missed.")
  }
  
    # Apply conditions directly without a function
    data_21 <- data_21 %>%
      mutate(pgy = case_when(
        author %in% resident.names$`PGY.3` ~ "pgy1.21.22",
        author %in% resident.names$`PGY.4` ~ "pgy2.21.22",
        author %in% resident.names$`PGY.5` ~ "pgy3.21.22",
        TRUE ~ NA_character_
      ))
    # Apply conditions directly without a function
    data_22 <- data_22 %>%
      mutate(pgy = case_when(
        author %in% resident.names$`PGY.2` ~ "pgy1.22.23",
        author %in% resident.names$`PGY.3` ~ "pgy2.22.23",
        author %in% resident.names$`PGY.4` ~ "pgy3.22.23",
        TRUE ~ NA_character_
      ))
    # Apply conditions directly without a function
    data_23 <- data_23 %>%
      mutate(pgy = case_when(
        author %in% resident.names$`PGY.1` ~ "pgy1.23.24",
        author %in% resident.names$`PGY.2` ~ "pgy2.23.24",
        author %in% resident.names$`PGY.3` ~ "pgy3.23.24",
        TRUE ~ NA_character_
      ))
  
  # join the 3 dataframes, leaving out those outside the date range
  all_data_8 <- bind_rows(data_21, data_22, data_23)

  
  outside_time_range_count = nrow(all_data_7) - nrow(data_21) - nrow(data_22) - nrow(data_23)
  print(outside_time_range_count)
  
  # Remove these rows from the original data frame
  all_data_9 <- all_data_8 %>%
    filter(
      (`edittime(seconds)` > 2 & !author %in% staffs) |  # Handle edittime and author conditions
      (`edittime(seconds)` > 0 & author %in% staffs)   # Handle edittime and author conditions
    )
  
  editing_time_insufficient_count = nrow(all_data_8) - nrow(all_data_9)
  print(editing_time_insufficient_count)
  
  all_data_9 <- all_data_9 %>%
    filter((is.na(notesize) | notesize != 0))    # Keep notesize that are NA or not zero

  empty_note_count = nrow(all_data_8) - editing_time_insufficient_count - nrow(all_data_9)
  print(empty_note_count)

    # These are unaccounted for since they are overlapped between the criteria above (e.g. <5 seconds and before 6/30/21)
      removed = - nrow(all_data_9) + nrow(all_data_8)
      print(removed)

    summary.7 <- sum_df(all_data_9)

# Delete all notes where a resident did not have a true contribution (<5 seconds). We will group the dataframe by note id, then look at all the editing instances of each unique note id, and if the pgy column do not include one of the residents (all its values are), then will delete these note ids

filtered_data <- all_data_9 %>%
  group_by(noteid) %>%
  filter(!all(is.na(pgy))) %>%
  ungroup()

# Columns to be deleted
cols_to_delete <- c("birthdate", "day", "editstart", "editstop", 
                    "noteauthorlogindept", "noteauthornpi", "noteduration(physician)", 
                    "noteduration(resident)", "shift", "specialtyatnote", 
                    "specialtycurrent", "timediff(hr)")

# Delete specified columns
filtered_data <- filtered_data %>% select(-all_of(cols_to_delete))

  cons.data <- filtered_data %>%
    group_by(noteid) %>%
    summarise(
      total.time = sum(`edittime(seconds)`),
      weekend = sum(weekend, na.rm = TRUE),
      after.hours = sum(after.hours, na.rm = TRUE),
      pm = sum(pm, na.rm = TRUE),
      across(everything(), first),
      .groups = "drop"
    )
  
  cons.data <- cons.data %>%
  mutate(pm = ifelse(encountercontext != "Inpatient" & pm == 0, NA, pm),
         after.hours = ifelse(encountercontext != "Ambulatory" & after.hours == 0, NA, after.hours))
  
# 2- attendings file (consolidated by ID, all attending edits for each note combined), delete unneeded columns
  staff.data <- filtered_data %>%
    filter(author %in% staffs) %>%
    group_by(noteid) %>%
    summarise(staff.time = sum(`edittime(seconds)`), 
              across(everything(), first),
              .groups = "drop")

# 3- all residents file (consolidated by ID and class/resident, all notes), delete unneeded columns
  all.resident.data <- filtered_data %>%
    filter(pgy != "NA") %>%
    group_by(noteid) %>%
    summarise(resident.time = sum(`edittime(seconds)`), 
              across(everything(), first),
              .groups = "drop")

# Join the resident and staff eidting time columns to the main dataframe
  # Join staff.time from staff.data
  cons.data <- cons.data %>%
    left_join(staff.data %>% select(noteid, staff.time), by = "noteid")
  
  # Join resident.time from resident.data
  cons.data <- cons.data %>%
    left_join(all.resident.data %>% select(noteid, resident.time), by = "noteid")
  
  # Add contribution of resident percentage column
  cons.data <- cons.data %>%
    mutate(resident.percent = resident.time / total.time * 100)
  
# 4- Individual resident file (consolidated by residents)
  per.resident <- filtered_data %>%
    filter(pgy != "NA") %>%
    group_by(noteid, author) %>%
    summarise(
      resident.time = sum(`edittime(seconds)`),
      weekend = sum(weekend, na.rm = TRUE),
      after.hours = sum(after.hours, na.rm = TRUE),
      pm = sum(pm, na.rm = TRUE),
      across(everything(), first),
      .groups = "drop"
    )
  
  per.resident <- per.resident %>%
  mutate(pm = ifelse(encountercontext != "Inpatient" & pm == 0, NA, pm),
         after.hours = ifelse(encountercontext != "Ambulatory" & after.hours == 0, NA, after.hours))
  
# Summary statement
summary_messages <- c(
  paste("At the beginning, there were", nrow(all_data_1), "notes. Out of these, notes with equal or incomplete edit time data counted", nrow(not_equal), "and", nrow(no_times), ", respectively.", na_count, "notes had incorrect times to calculate a start-to-stop value. The file was then separated for each edit instance resulting in", nrow(all_data_3), "instances, of which", nrow(all_data_8) - nrow(all_data_9), "were removed because these lasted <5 seconds and were not attending edits or lasted <1 second and were attending edits, or contained 0 characters.", nrow(before) + nrow(after), "editing instances were removed because they occurred before 7/1/21 or after 6/30/24. Further", nrow(all_data_9) - nrow(filtered_data), "editing instances were removed since there were no true resident contributions to the associated notes. The final note count after refinements is", nrow(cons.data), "meaning that", (nrow(all_data_2) - nrow(cons.data)), "notes were excluded while editing instances were refined."),
  paste("Total number of excluded notes is (both numbers should be equal):", nrow(not_equal) + nrow(no_times) + (nrow(all_data_2) - nrow(cons.data)), nrow(all_data_1) - nrow(cons.data))
)

# To print the messages, you can then do:
print(summary_messages[1])  # Print first message
print(summary_messages[2])  # Print second message
      
saveRDS(filtered_data, file = "all_data.rds")
saveRDS(cons.data, file = "cons_data.rds")
saveRDS(per.resident, file = "per.resident.rds")
saveRDS(summary_messages, file = "summary_messages.rds")

```


```{r, EDA & CLT, echo=FALSE}

# read the main file
cons.data <- readRDS("cons_data.rds")
k = sum_df(cons.data)

# Create a new column "other.time"
cons.data <- cons.data %>%
  mutate(other.time = total.time - staff.time - resident.time)

# Change "smartphraseids" into a numerical column by counting the number of occurrences of "["
cons.data <- cons.data %>%
  mutate(smartphraseids = str_count(smartphraseids, "\\["))

# Define the columns to be plotted
columns1 <- c("notetype", "cosigninformation", "notestatus", "authorsservice", "encountercontext", "sharedwithpatient")
columns2 <- c("reasonforblockingnote", "ccas", "pgy")

# Create and save plots
create_and_save_plots(cons.data, columns1, "combined_plot1")
create_and_save_plots(cons.data, columns2, "combined_plot2")

# Create and save plots for numeric variables
create_and_save_numeric_plots(cons.data, "numeric_plot")

#Given the very large number of measurements in the dataset, the data, untransformed, vs the data after several different transformations was tested as to whether it meets the central limit theorem (CLT, i.e. 1000 samples, each of 500 measurements - reflecting the large subsets we would work with - were taken and the means were QQ plotted and tested for normality using the Anderson-Darling normality and Shapiro-Walk normality tests). This was done to the 9 numeric variables below and the log transformation, with its ease of use, was thought to be the best transformation to use as all the transformed variables met normality estimate and therefore the CLT was assumed to apply for each.

# Define the columns to be analyzed
columns <- c("total.time", "resident.time", "staff.time", "other.time", "notesize", "%copied", "%voice", "starttostop", "smartphraseids", "resident.percent", "pm", "weekend", "after.hours")

# Perform analysis (results is a dataframe with normality test results for each transformation and each column)
results <- analyze_transformations(cons.data, columns)
#print(results)


# Filter and summarize data for each note type
plot_data <- cons.data %>%
  filter(notetype %in% c("Clinic Note", "Admission Note", "Progress Note", "Discharge Summary")) %>%
  group_by(month = floor_date(dtofsvc, "month"), notetype) %>%
  summarise(mean_time = mean(total.time), .groups = 'drop')

# Plotting based on dtofsvc
plot_dtofsvc <- ggplot(plot_data, aes(x = month, y = mean_time, color = notetype)) +
  geom_line() +
  labs(title = "Monthly Trend of Total Time by Note Type (dtofsvc)",
       x = "Month",
       y = "Average Total Time (min)",
       color = "Note Type") +
  theme_minimal()

# Save the plot to working directory
ggsave("plot_dtofsvc.png", plot = plot_dtofsvc, width = 10, height = 6, dpi = 300)

# Adjust data filtering and summarization for filetime
plot_data_filetime <- cons.data %>%
  filter(notetype %in% c("Clinic Note", "Admission Note", "Progress Note", "Discharge Summary")) %>%
  group_by(month = floor_date(filetime, "month"), notetype) %>%
  summarise(mean_time = mean(total.time), .groups = 'drop')

# Plotting based on filetime
plot_filetime <- ggplot(plot_data_filetime, aes(x = month, y = mean_time, color = notetype)) +
  geom_line() +
  labs(title = "Monthly Trend of Total Time by Note Type (filetime)",
       x = "Month",
       y = "Average Total Time (min)",
       color = "Note Type") +
  theme_minimal()

# Save the plot to working directory
ggsave("plot_filetime.png", plot = plot_filetime, width = 10, height = 6, dpi = 300)

```


```{r, per resident and figure 1, echo=false}

per.resident <- readRDS("per.resident.rds")

pgy1 <- per.resident %>%
  filter(pgy %in% c("pgy1.21.22", "pgy1.22.23", "pgy1.23.24"))

pgy2 <- per.resident %>%
  filter(pgy %in% c("pgy2.21.22", "pgy2.22.23", "pgy2.23.24"))

pgy3 <- per.resident %>%
  filter(pgy %in% c("pgy3.21.22", "pgy3.22.23", "pgy3.23.24"))

# Step 2: Split the data frame into a list of data frames based on unique values in 'author' column
listpgy1 <- split(pgy1, pgy1$author)
listpgy2 <- split(pgy2, pgy2$author)
listpgy3 <- split(pgy3, pgy3$author)

# Apply the function to each dataframe in the list and combine the results
per.pgy1 <- do.call(rbind, lapply(listpgy1, calculate_statistics))
per.pgy2 <- do.call(rbind, lapply(listpgy2, calculate_statistics))
per.pgy3 <- do.call(rbind, lapply(listpgy3, calculate_statistics))

# Add a PGY level indicator to each dataframe
per.pgy1$PGY <- 'PGY1'
per.pgy2$PGY <- 'PGY2'
per.pgy3$PGY <- 'PGY3'

# Combine the dataframes into one
combined_df <- bind_rows(per.pgy1, per.pgy2, per.pgy3)

# Create plots for Time-related, Count-related, and Shift-related variables
time_plot <- create_melted_plot(combined_df, c("Overall", "HP", "Clinic", "Discharge", 
                                               "Consult", "Progress", "Telephone", 
                                               "ED", "Procedure"), 
                                "Average Time per Note Type", "Type of Note", "Time in Minutes")

count_plot <- create_melted_plot(combined_df, c("All.Notes", "Inpatient", "Ambulatory"), 
                                 "Counts of Notes by Context", "Context of Note", "Count of Notes")

shift_plot <- create_melted_plot(combined_df, c("Weekend", "Inpatient.PM", "Ambulatory.After_Hours"), 
                                 "Percentage of Notes During Specific Shifts", "Shift Type", "Percentage (%)")

# Select relevant columns for correlation analysis and handle missing values
correlation_data <- na.omit(combined_df[, c("PGY", "Overall", "Percent.Copied", "Percent.Dictated", 
                                            "Days.to.Sign", "Note.Size")])

# Melt data for correlation plots
correlation_data_melted <- melt(correlation_data, id.vars = c("PGY", "Overall"), variable.name = "Variable", value.name = "Value")

# Create correlation plot with smaller font and points
correlation_plot <- ggplot(correlation_data_melted, aes(x = Overall, y = Value, color = PGY)) +
  geom_point(alpha = 0.6, size = 1.5) +
  geom_smooth(aes(group = PGY), method = "lm", se = FALSE, fullrange = TRUE, linetype = "dashed") +
  facet_wrap(~ Variable, scales = "free_y") +
  labs(title = "Correlation between Mean Time and Various Metrics", x = "Mean Time in Minutes", y = "Value") +
  theme_minimal() +
  theme(axis.text.x = element_text(angle = 45, hjust = 1, size = 8),
        axis.text.y = element_text(size = 8),
        strip.text = element_text(size = 10, face = "bold"),
        legend.position = "right",
        legend.text = element_text(size = 8),
        legend.title = element_text(size = 10))

# Arrange the plots in a 2x2 matrix and save the combined plot
combined_plot <- grid.arrange(time_plot, count_plot, shift_plot, correlation_plot, ncol = 2, nrow = 2)
ggsave("figure.1.individual.variability.png", plot = combined_plot, width = 14, height = 12)


```


```{r, per block, echo=FALSE}

block_start_dates <- c(as.Date("2021-07-01"), seq(as.Date("2021-08-02"), as.Date("2024-06-30"), by = 28))
split_data_list <- list()

for (i in seq_along(block_start_dates)){
  if (i < length(block_start_dates)) {
    split_data_list[[i]] <- cons.data %>%
      filter(dtofsvc >= block_start_dates[i] & dtofsvc < block_start_dates[i + 1])
  } else {
    # For the last block, take all dates from the last date to the end of the range
    split_data_list[[i]] <- cons.data %>%
      filter(dtofsvc >= block_start_dates[i])
  }
}
#change
block_data <- do.call(rbind, lapply(split_data_list, calculate_statistics))

# 1. total time vs resident time vs attending time (changes in sum in days over blocks)
# 2. PGY 1 vs 2 vs 3 (changes in sum in days over blocks)
# 3. HP vs Clinic note vs DS vs consult notes (changes in sum in days over blocks)
# 4. Ambulatory vs ED vs Inpatient (changes in sum in days over blocks)
# 5. pm vs after.hours vs nothing

# # Create the plot with adjusted colors and theme for a black background
# plot <- ggplot(combined_df, aes(x = Block, y = MeanEditTime, color = Group, group = Group)) +
#   geom_line(size = 1) +  # Line for each group
#   geom_point(size = 2) +  # Points for each group
#   geom_text(aes(label=NoteCount), vjust=-1.5, color="white", size = 2) +  # Add note count labels
#   labs(title = "Trend of Mean Edit Time Across Blocks", x = "Block", y = "Mean Edit Time (mins)") +
#   scale_y_continuous(limits = c(60, 120)) +  # Set y-axis limits
# 
#   # Add vertical lines for PDSA cycles, arrows, and other annotations as before
#   geom_vline(xintercept = 16, linetype="dashed", color = "#FFD700") +
#   annotate("text", x = 16, y = 115, label = "PDSA Cycle 1", angle = 90, vjust = -0.5, color = "white") +
#   geom_vline(xintercept = 27, linetype="dashed", color = "#FFD700") +
#   annotate("text", x = 27, y = 115, label = "PDSA Cycle 2", angle = 90, vjust = -0.5, color = "white") +
#   annotate("segment", x = 14, xend = 14, y = 65, yend = 60, colour = "#FF6347", size=0.5, arrow=arrow()) +
#   annotate("text", x = 14.7, y = 66, label = "22-23", hjust = 1, color = "white") +
#   annotate("segment", x = 27, xend = 27, y = 65, yend = 60, colour = "#FF6347", size=0.5, arrow=arrow()) +
#   annotate("text", x = 27.8, y = 66, label = "23-24", hjust = 1, color = "white") +
# 
#   theme_minimal(base_family = "CMU Serif") + 
#   theme(
#     text = element_text(color = "white"),
#     plot.background = element_rect(fill = "black"),
#     panel.background = element_rect(fill = "black"),
#     panel.grid.major = element_line(color = "#666666"),
#     panel.grid.minor = element_line(color = "#666666"),
#     plot.title = element_text(margin = margin(b = 20)),
#     axis.title.x = element_text(margin = margin(t = 10)),
#     axis.title.y = element_text(margin = margin(r = 10))
#   )
# 
# # Save the plot
# ggsave("MeanEditTimeTrend.Combined.png", plot, width = 14, height = 8, dpi = 300, bg = "black")

```


```{r, correlations}

cons.data <- readRDS("cons_data.rds")

# Applying multiple transformations in a single pipeline
cons.data <- cons.data %>%
  # Count "[" in smartphraseids to get the number of smart phrases used
  mutate(smart_phrases_count = str_count(smartphraseids, "\\[")) %>%
  
  # Calculate 'other.time' as 'total.time' minus 'staff.time' and 'resident.time'
  mutate(other.time = total.time - staff.time - resident.time) %>%
  
  # Create 'outside.work.hours' by combining 'pm' and 'weekend' where the first non-NA value is chosen
  mutate(outside.work.hours = coalesce(pm, after.hours)) %>%
  
  # Apply log1p transformation to multiple variables
  mutate_at(vars(outside.work.hours, staff.time, other.time, starttostop, smart_phrases_count),
            ~ log1p(.))  # log1p to handle zeros by transforming log(1 + x)

# Select relevant columns for further analysis
selected_data <- cons.data %>%
  select(total.time, `%copied`, `%voice`, outside.work.hours, notesize, smart_phrases_count,
         resident.time, staff.time, other.time, starttostop, resident.percent, weekend)

# Calculate correlation matrix
correlation_matrix <- cor(selected_data, method = "pearson", use = "complete.obs")  # Handles missing values by listwise deletion

print(correlation_matrix)

write.csv(correlation_matrix, "correlation_matrix.csv")

# Melting the correlation matrix into a long format
melted_corr_matrix <- melt(correlation_matrix)

# Plotting the heatmap
x = ggplot(data = melted_corr_matrix, aes(x = Var1, y = Var2, fill = value)) +
  geom_tile() +
  geom_text(aes(label = sprintf("%.2f", value)), size = 3, color = "black") +
  scale_fill_gradient2(low = "blue", high = "red", mid = "white", midpoint = 0, limit = c(-1,1)) +
  theme_minimal() +
  theme(axis.text.x = element_text(angle = 45, hjust = 1),
        axis.text.y = element_text(angle = 0, hjust = 1)) +
  labs(title = "Heatmap of Pearson Correlation with Coefficients", x = "", y = "", fill = "Correlation")
ggsave("xx.png", plot = x, width = 14, height = 12)

# Creating the matrix of scatter plots with slope lines in the lower triangle
c <- ggpairs(selected_data,
             columns = c("total.time", "%copied", "%voice", "outside.work.hours", "notesize", "resident.time", "staff.time", "starttostop", "resident.percent", "weekend", "other.time"),
             lower = list(continuous = function(data, mapping, ...) {
                 ggplot(data = data, mapping = mapping) + 
                     geom_point(color = "blue") +
                     geom_smooth(method = "lm", color = "red", se = FALSE)
             }),
             upper = list(continuous = "cor"),
             diag = list(continuous = "barDiag"))

# Saving the plot
ggsave("correlation.matrix.png", plot = c, width = 14, height = 14, dpi = 600, device = "png")


```


```{r, note time by note type and contributor, echo=FALSE}

cons.data <- readRDS("cons_data.rds")

# Applying multiple transformations in a single pipeline
cons.data <- cons.data %>%
  mutate(other.time = total.time - staff.time - resident.time)

# Create a table of note types
note_type_table <- table(cons.data$notetype)

# Convert the table to a dataframe
typecounts <- data.frame(
  NoteType = names(note_type_table),
  Counts = as.integer(note_type_table)
)

# Sort the dataframe by counts in descending order and keep the top 10
typecounts <- typecounts[order(-typecounts$Counts), ]
typecounts <- head(typecounts, 10)

# Extract the NoteType column to create a character vector of allowed note types (highest 10 in frequency, excluding flowsheet notes)
allowed_notetypes <- c("Progress Notes", "Clinic Note", "Discharge Summary", "Admission Note", "Emergency Department", "Procedures", "Telephone Encounter", "Event", "Consults")

# Define the list of combinations to analyze
combinations <- list(
  list(cont_var = "resident.time", cat_var = "notetype"),
  list(cont_var = "other.time", cat_var = "notetype"),
  list(cont_var = "staff.time", cat_var = "notetype")
)

# Apply the perform_analysis function to each combination
results <- lapply(combinations, function(combo) {
  df <- cons.data  # Ensure cons.data is the dataframe you want to analyze
  tryCatch({
    result <- perform_analysis(df, combo$cont_var, combo$cat_var)
    list(variable = combo$cont_var, category = combo$cat_var, data = result)
  }, error = function(e) {
    message("Error processing ", combo$cont_var, " by ", combo$cat_var, ": ", e$message)
    NULL  # Returning NULL in case of error to keep the results clean
  })
})

# Assuming 'results' is your list of lists
combined_df <- bind_rows(
  lapply(results, function(x) {
    data <- x$data
    data$variable <- x$variable
    data$category <- x$category
    return(data)
  }),
  .id = "source_id"  # This optional line adds an ID for each source data frame
)

modified_df <- combined_df %>%
  select(variable, group1, group2, N2, Real_Mean2, LCI2, UCI2, p.adj, p.adj.signif, estimate, conf.low, conf.high, Mean_Difference, CI_Real_Mean2_Lower, CI_Real_Mean2_Upper) %>%
  distinct()

# Filter rows where 'group2' is in the allowed notetypes
filtered_df <- modified_df %>%
  filter(group2 %in% allowed_notetypes) %>%
  distinct()

# Adjusting the time units from seconds to minutes for 'Real_Mean2' and CI bounds
result_df <- filtered_df %>%
  group_by(group2, variable) %>%
  mutate(Real_Mean2 = Real_Mean2 / 60,
         LCI2 = LCI2 / 60,
         UCI2 = UCI2 / 60,
         Mean_Difference = Mean_Difference / 60,
         CI_Real_Mean2_Lower = CI_Real_Mean2_Lower / 60,
         CI_Real_Mean2_Upper = CI_Real_Mean2_Upper / 60) %>%
  slice_min(order_by = CI_Real_Mean2_Upper - CI_Real_Mean2_Lower) %>%
  ungroup()

# Renaming the variables for clarity
result_df$variable <- dplyr::recode(result_df$variable,
                             "resident.time" = "Resident",
                             "staff.time" = "Attending",
                             "other.time" = "Other")

# Create a new label that combines note type and count
typecounts$Label <- paste(typecounts$NoteType, " (", typecounts$Counts, ")", sep="")

# Sort 'typecounts' by 'Counts' in descending order
typecounts <- typecounts[order(-typecounts$Counts), ]

# Merge this label with the result_df dataframe based on NoteType
result_df_2 <- merge(result_df, typecounts, by.x="group2", by.y="NoteType")

# Set the factor levels for 'Label' in 'result_df' based on the order in 'typecounts'
result_df_2$Label <- factor(result_df$Label, levels = typecounts$Label)

# Update the plotting code to use the new labels with ordered levels
# # Whisker Plot of Average Note Time by Author's Type & Note Type
g2 <- ggplot(result_df_2, aes(x = Label, y = Real_Mean2, group = variable, color = variable, shape = variable)) +
  geom_point(position = position_dodge(width = 0.5)) +  # Position dodge to separate the points
  geom_errorbar(aes(ymin = CI_Real_Mean2_Lower, ymax = CI_Real_Mean2_Upper), 
                position = position_dodge(width = 0.5), width = 0.1) +
  geom_text(aes(label = paste("N =", N2), y = 60),  # Adjust y position to minimize overlap
            position = position_dodge(width = 0.5), angle = 90, vjust = -0.5, hjust = 1, 
            size = 2.0) +  # Smaller text size
  labs(title = "A: Note Type & Author Type",
       x = "Note Type (Count)",
       y = "Average Time (min)",
       color = "Author Type",
       shape = "Author Type") +
  theme_minimal() +
  theme(axis.text.x = element_text(angle = 45, hjust = 1),  # Rotate x-axis labels for better readability
        axis.title.y = element_text(angle = 90, vjust = 0.5, margin = margin(r = 20, unit = "pt"))) +  # Rotate y-axis label to vertical and add space
  ylim(0, 62)  # Dynamically set y-axis limit to accommodate text

print(g2)

ggsave("figure2.png", plot = g2, width = 10, height = 6, dpi = 600, device = "png")

### NOW MAKE GRAPHS (WHISKER PLOTS) FOR EACH GROUP OF DATA: note type, authors service, PGY, CCAs, collapsible

```


```{r, note time by service and contributor, echo=FALSE}

cons.data <- readRDS("cons_data.rds")

# Applying multiple transformations in a single pipeline
cons.data <- cons.data %>%
  mutate(other.time = total.time - staff.time - resident.time)

# Recode specific labels in 'result_df$Label'
cons.data$authorsservice <- dplyr::recode(cons.data$authorsservice,
                          "Critical Care PICU" = "PICU",
                          "Critical Care NICU" = "NICU",
                          "Center for Disabilities and Development" = "CDD")

# Create a table of note types
note_type_table <- table(cons.data$notetype)

# Create a table of note services
service_table <- table(cons.data$authorsservice)

# Convert the table to a dataframe
servicecounts <- data.frame(
  NoteService = names(service_table),
  Counts = as.integer(service_table)
)

# Sort the dataframe by counts in descending order and keep the top 10
servicecounts <- servicecounts[order(-servicecounts$Counts), ]
servicecounts <- head(servicecounts, 14)

# Extract the NoteType column to create a character vector of allowed note types
allowed_noteservice <- as.character(servicecounts$NoteService)

# Define the list of combinations to analyze
combinations <- list(
  list(cont_var = "resident.time", cat_var = "authorsservice"),
  list(cont_var = "other.time", cat_var = "authorsservice"),
  list(cont_var = "staff.time", cat_var = "authorsservice")
)

# Apply the perform_analysis function to each combination
results <- lapply(combinations, function(combo) {
  df <- cons.data  # Ensure cons.data is the dataframe you want to analyze
  tryCatch({
    result <- perform_analysis(df, combo$cont_var, combo$cat_var)
    list(variable = combo$cont_var, category = combo$cat_var, data = result)
  }, error = function(e) {
    message("Error processing ", combo$cont_var, " by ", combo$cat_var, ": ", e$message)
    NULL  # Returning NULL in case of error to keep the results clean
  })
})

# Assuming 'results' is your list of lists
combined_df <- bind_rows(
  lapply(results, function(x) {
    data <- x$data
    data$variable <- x$variable
    data$category <- x$category
    return(data)
  }),
  .id = "source_id"  # This optional line adds an ID for each source data frame
)


# Select needed columns and Filter rows where 'group2' is in the allowed notetypes, then adjusting the time units from seconds to minutes for 'Real_Mean2' and CI bounds

result_df <- combined_df %>%
  select(variable, group1, group2, N2, Real_Mean2, LCI2, UCI2, p.adj, p.adj.signif, estimate, conf.low, conf.high, Mean_Difference, CI_Real_Mean2_Lower, CI_Real_Mean2_Upper) %>%
  distinct() %>%
  filter(group2 %in% allowed_noteservice) %>%
  group_by(group2, variable) %>%
  mutate(Real_Mean2 = Real_Mean2 / 60,
         LCI2 = LCI2 / 60,
         UCI2 = UCI2 / 60,
         Mean_Difference = Mean_Difference / 60,
         CI_Real_Mean2_Lower = CI_Real_Mean2_Lower / 60,
         CI_Real_Mean2_Upper = CI_Real_Mean2_Upper / 60) %>%
  slice_min(order_by = CI_Real_Mean2_Upper - CI_Real_Mean2_Lower) %>%
  ungroup()

# Renaming the variables for clarity
result_df$variable <- dplyr::recode(result_df$variable,
                             "resident.time" = "Resident",
                             "staff.time" = "Attending",
                             "other.time" = "Other")

# Create a new label that combines note type and count
servicecounts$Label <- paste(servicecounts$NoteService, " (", servicecounts$Counts, ")", sep="")

# Sort 'typecounts' by 'Counts' in descending order
servicecounts <- servicecounts[order(-servicecounts$Counts), ]

# Merge this label with the result_df dataframe based on NoteType
result_df <- merge(result_df, servicecounts, by.x="group2", by.y="NoteService")

# Set the factor levels for 'Label' in 'result_df' based on the order in 'typecounts'
result_df$Label <- factor(result_df$Label, levels = servicecounts$Label)

# Update the plotting code to use the new labels with ordered levels
# # Whisker Plot of Average Note Time by Author's type & Author's Service
g3 <- ggplot(result_df, aes(x = Label, y = Real_Mean2, group = variable, color = variable, shape = variable)) +
  geom_point(position = position_dodge(width = 0.5)) +
  geom_errorbar(aes(ymin = CI_Real_Mean2_Lower, ymax = CI_Real_Mean2_Upper), 
                position = position_dodge(width = 0.5), width = 0.1) +
  geom_text(aes(label = paste("N =", N2), y = 70),  # Adjust y position to minimize overlap
            position = position_dodge(width = 0.5), angle = 90, vjust = -0.5, hjust = 1, 
            size = 2.0) +
  labs(title = "B: Author Type & Service",
       x = "Author Service (Count)",
       y = "Average Time (min)",
       color = "Author Type",
       shape = "Author Type") +
  theme_minimal() +
  theme(axis.text.x = element_text(angle = 45, hjust = 1),  # Rotate x-axis labels for better readability
        axis.title.y = element_text(angle = 90, vjust = 0.5, margin = margin(r = 20, unit = "pt"))) +  # Rotate y-axis label to vertical and add space
  ylim(0, 72)  # Dynamically set y-axis limit to accommodate text

print(g3)

ggsave("figure3.png", plot = g3, width = 10, height = 5, dpi = 600, device = "png")

### NOW MAKE GRAPHS (WHISKER PLOTS) FOR EACH GROUP OF DATA: note type, authors service, PGY, CCAs, collapsible

```


```{r, resident time by note type and pgy, echo=FALSE}

per.resident <- readRDS("per.resident.rds")

# Recode specific labels in 'result_df$Label'
per.resident$pgy <- dplyr::recode(per.resident$pgy,
                          "pgy1.21.22" = "PGY-1",
                          "pgy1.22.23" = "PGY-1",
                          "pgy1.23.24" = "PGY-1",
                          "pgy2.21.22" = "PGY-2",
                          "pgy2.22.23" = "PGY-2",
                          "pgy2.23.24" = "PGY-2",
                          "pgy3.21.22" = "PGY-3",
                          "pgy3.22.23" = "PGY-3",
                          "pgy3.23.24" = "PGY-3")

# Data preprocessing
per.resident <- per.resident %>%
  select(notetype, pgy, resident.time) %>%
  mutate(resident.time = ifelse(resident.time <= 0, NA, resident.time)) %>%
  filter(!is.na(resident.time)) %>%
  mutate(log_transformed = log(resident.time))

# Create an interaction term
per.resident <- per.resident %>%
  mutate(notetype_pgy = interaction(notetype, pgy))

# Descriptive statistics on log-transformed data
descriptive_stats <- per.resident %>%
  group_by(notetype, pgy) %>%
  summarise(
    Count_Non_NA = n(),
    Real_Mean = exp(mean(log_transformed, na.rm = TRUE)),
    sd = exp(sd(log_transformed, na.rm = TRUE)),
    N = sum(!is.na(log_transformed)),
    sd_log = sd(log_transformed, na.rm = TRUE),
    SE = exp(sd_log) / sqrt(N),
    ci_lower = exp(mean(log_transformed, na.rm = TRUE) - qt(0.975, df = N - 1) * sd_log / sqrt(N)),
    ci_upper = exp(mean(log_transformed, na.rm = TRUE) + qt(0.975, df = N - 1) * sd_log / sqrt(N)),
    .groups = 'drop'
  )

# Filter data for groups that have at least 2 observations
per.resident <- per.resident %>%
  group_by(notetype, pgy) %>%
  filter(n() >= 2) %>%
  ungroup()

# Two-way Welch's ANOVA on log-transformed data
if (nrow(per.resident) > 1) {
  anova_results <- welch_anova_test(per.resident, log_transformed ~ notetype * pgy)
}

# Games-Howell post-hoc test on log-transformed data
post_hoc_results <- per.resident %>%
  games_howell_test(log_transformed ~ notetype_pgy)

  # Reversing groups and adjusting corresponding values
  reversed_post_hoc_results <- post_hoc_results %>%
  mutate(
    # Swap groups
    old_group1 = group1,
    group1 = group2,
    group2 = old_group1,
    
    # Reverse the signs of estimates and confidence intervals
    estimate = -estimate,
    old_conf_low = conf.low,
    conf.low = -conf.high,
    conf.high = -old_conf_low
  ) %>%
  select(-old_group1, -old_conf_low)  # Removing temporary columns

  # Combine the original and reversed data frames
  combined_results <- bind_rows(post_hoc_results, reversed_post_hoc_results) %>%
  distinct

# Assuming allowed_notetypes is already defined as you mentioned
allowed_notetypes <- c("Progress Notes", "Clinic Note", "Discharge Summary", "Admission Note", "Emergency Department", "Procedures", "Telephone Encounter", "Event", "Consults")
  
post_hoc_results <- combined_results %>%
  separate(group2, into = c("notetype2_split", "pgy2_split"), sep = "\\.", extra = "merge") %>%
  separate(group1, into = c("notetype1_split", "pgy1_split"), sep = "\\.", extra = "merge")

# Merge to add Real Means
post_hoc_results <- post_hoc_results %>%
  left_join(descriptive_stats %>%
              rename(group1_notetype = notetype, group1_pgy = pgy, N1 = N, LCI1 = ci_lower, UCI1 = ci_upper, sd1 = sd, se1 = SE, Real_Mean1 = Real_Mean) %>%
              select(group1_notetype, group1_pgy, N1, LCI1, UCI1, sd1, se1, Real_Mean1), 
            by = c("notetype1_split" = "group1_notetype", "pgy1_split" = "group1_pgy")) %>%
  left_join(descriptive_stats %>%
              rename(group2_notetype = notetype, group2_pgy = pgy, N2 = N, LCI2 = ci_lower, UCI2 = ci_upper, sd2 = sd, se2 = SE, Real_Mean2 = Real_Mean) %>%
              select(group2_notetype, group2_pgy, N2, LCI2, UCI2, sd2, se2, Real_Mean2), 
            by = c("notetype2_split" = "group2_notetype", "pgy2_split" = "group2_pgy")) %>%
  mutate(
    Mean_Difference = Real_Mean2 - Real_Mean1,
    Estimated_Real_Mean2 = Real_Mean1 * exp(estimate),
    CI_Real_Mean2_Lower = Real_Mean1 * exp(conf.low),
    CI_Real_Mean2_Upper = Real_Mean1 * exp(conf.high)
  )

modified_df <- post_hoc_results %>%
  select(notetype1_split, pgy1_split, notetype2_split, pgy2_split, N2, Real_Mean2, LCI2, UCI2, p.adj, p.adj.signif, estimate, conf.low, conf.high, Mean_Difference, CI_Real_Mean2_Lower, CI_Real_Mean2_Upper) %>%
  distinct() %>%
  filter(notetype2_split %in% allowed_notetypes) %>%
  group_by(notetype2_split, pgy2_split) %>%
  mutate(Real_Mean2 = Real_Mean2 / 60,
         LCI2 = LCI2 / 60,
         UCI2 = UCI2 / 60,
         Mean_Difference = Mean_Difference / 60,
         CI_Real_Mean2_Lower = CI_Real_Mean2_Lower / 60,
         CI_Real_Mean2_Upper = CI_Real_Mean2_Upper / 60) %>%
  slice_min(order_by = CI_Real_Mean2_Upper - CI_Real_Mean2_Lower) %>%
  ungroup()

# Create a table of note types
note_type_table <- table(cons.data$notetype)

# Convert the table to a dataframe
typecounts <- data.frame(
  NoteType = names(note_type_table),
  Counts = as.integer(note_type_table)
)

# Create a new label that combines note type and count
typecounts$Label <- paste(typecounts$NoteType, " (", typecounts$Counts, ")", sep="")

# Sort 'typecounts' by 'Counts' in descending order
typecounts <- typecounts[order(-typecounts$Counts), ]

# Merge this label with the result_df dataframe based on NoteType
modified_df <- merge(modified_df, typecounts, by.x="notetype2_split", by.y="NoteType")

# Set the factor levels for 'Label' in 'result_df' based on the order in 'typecounts'
modified_df$Label <- factor(modified_df$Label, levels = typecounts$Label)

# Update the plotting code to use the new labels with ordered levels
# # Whisker Plot of Average Resident Note Time by Note Type & PGY
g4 <- ggplot(modified_df, aes(x = Label, y = Real_Mean2, group = pgy2_split, color = pgy2_split, shape = pgy2_split)) +
  geom_point(position = position_dodge(width = 0.5)) +  # Position dodge to separate the points
  geom_errorbar(aes(ymin = CI_Real_Mean2_Lower, ymax = CI_Real_Mean2_Upper), 
                position = position_dodge(width = 0.5), width = 0.1) +
  geom_text(aes(label = paste("N =", N2), y = 60),  # Adjust y position to minimize overlap
                position = position_dodge(width = 0.5), angle = 90, vjust = -0.5, hjust = 1, 
                size = 2.0) +
  labs(title = "C: Note Type & Resident Class",
       x = "Note Type (Count)",
       y = "Average Time (min)",
       color = "Resident Class",
       shape = "Resident Class") +
  theme_minimal() +
  theme(axis.text.x = element_text(angle = 45, hjust = 1),  # Rotate x-axis labels for better readability
        axis.title.y = element_text(angle = 90, vjust = 0.5, margin = margin(r = 20, unit = "pt"))) +  # Rotate y-axis label to vertical and add space
  ylim(0, 62)  # Dynamically set y-axis limit to accommodate text

print(g4)
ggsave("figure4.png", plot = g4, width = 10, height = 6, dpi = 600)

```


```{r, mergeing the 3 whiskers}

# Assuming g2, g3, and g4 are your ggplot objects
combined_plot <- g2 / g3 / g4 + 
  plot_annotation(title = "Whisker Plots of Average Note Time, 95% CI and Count",
                  theme = theme(plot.title = element_text(hjust = 0.5))) # Center the title

# Print the combined plot
print(combined_plot)

ggsave("whiskers.png", plot = combined_plot, width = 10, height = 18, dpi = 600, device = "png")


```


```{r, table 1, echo=FALSE}

# context vs type/service
cons.data <- readRDS("cons_data.rds")

summary_table <- cons.data %>%
  group_by(authorsservice, encountercontext, notetype) %>%
  summarise(count = n(), .groups = 'drop')

wide_table <- summary_table %>%
  pivot_wider(names_from = encountercontext, values_from = count, values_fill = list(count = 0)) %>%
  pivot_wider(names_from = notetype, values_from = c(`Ambulatory`, `Emergency Department`, `Inpatient`, `Telephone Encounter`))

# Assign 'notetype' as row names and then drop the column
wide_table$row_names <- wide_table$authorsservice
wide_table <- wide_table %>% 
  remove_rownames() %>%
  column_to_rownames(var = "row_names") %>%
  select(-authorsservice)

# Adding a Total row
total_row <- wide_table %>% 
  summarise(across(everything(), ~sum(.x, na.rm = TRUE)))

# Add the 'Total' row to the original dataframe
wide_table <- rbind(wide_table, total_row)
rownames(wide_table)[nrow(wide_table)] <- "Total"  # Naming the total row

# Adding a Total column
wide_table$total <- rowSums(wide_table, na.rm = TRUE)

# Step 1: Consolidate rows with total < 500 into "Other" row
# Identify rows with total < 500
low_total_rows <- rownames(wide_table)[wide_table$total < 500]

# Sum these rows into one row and remove them from the original data frame
other_row <- wide_table[low_total_rows, , drop = FALSE] %>%
  summarise(across(everything(), ~ sum(.x, na.rm = TRUE)))

# Add this new 'Other' row
wide_table <- wide_table[!rownames(wide_table) %in% low_total_rows, , drop = FALSE]
wide_table <- rbind(wide_table, other_row)
rownames(wide_table)[nrow(wide_table)] <- "Other"

# Step 2: Consolidate columns with total in 'Total' row < 500 into "Other" column
# Identify columns with total < 500
low_total_columns <- names(wide_table)[wide_table["Total", ] < 500]

# Sum these columns into one column and remove them from the original data frame
wide_table$Other <- rowSums(wide_table[, low_total_columns, drop = FALSE], na.rm = TRUE)
wide_table <- wide_table[, !names(wide_table) %in% low_total_columns]

# Sort the dataframe by the 'Total' column, descending order
wide_table <- wide_table %>% 
  arrange(desc(total))

# Transpose the dataframe
wide_table_transposed <- as.data.frame(t(wide_table))

# Convert row names into a column if needed for sorting
wide_table_transposed$categories <- rownames(wide_table_transposed)

# Sort by the 'Total' row, now a column after transposition
wide_table_transposed <- wide_table_transposed %>% 
  arrange(desc(`Total`))  # assuming 'Total' is now a column after transposition

# Optionally, transpose back if needed for presentation
wide_table_sorted <- t(wide_table_transposed)

# Convert back to data.frame if necessary
wide_table_sorted <- as.data.frame(wide_table_sorted)

# Calculate the grand total (excluding the 'Total' and 'Other' columns)
grand_total <- wide_table["Total", "total"]

# Convert 'Total' row and column to show both counts and percentages
wide_table["Total", -which(names(wide_table) %in% c("total", "Other"))] <- 
  sapply(wide_table["Total", -which(names(wide_table) %in% c("total", "Other"))], 
         function(x) sprintf("%d (%.2f%%)", x, x / grand_total * 100))

wide_table[-nrow(wide_table), "total"] <- 
  sapply(wide_table[-nrow(wide_table), "total"], 
         function(x) sprintf("%d (%.2f%%)", x, x / grand_total * 100))

# Convert 'Other' column to show both count and percentage for the 'Total' row
wide_table["Total", "Other"] <- 
  sprintf("%d (%.2f%%)", wide_table["Total", "Other"], wide_table["Total", "Other"] / grand_total * 100)

write.csv(wide_table, "table.1.csv")
writeLines(low_total_columns, "other.columns.txt")
writeLines(low_total_rows, "other.rows.txt")

```


```{r, summary}

resident_counts <- resident.names %>%
  summarise(across(everything(), ~ sum(!is.na(.))))

cons.data <- readRDS("cons_data.rds")
per.resident <- readRDS("per.resident.rds")
z = sum_df(cons.data)

# Applying multiple transformations in a single pipeline
cons.data <- cons.data %>%
  mutate(other.time = total.time - staff.time - resident.time)

inpatient <- cons.data %>%
  filter(encountercontext == "Inpatient")

ambulatory <- cons.data %>%
  filter (encountercontext == "Ambulatory")

inpatient.resident <- per.resident %>%
  filter(encountercontext == "Inpatient")

ambulatory.resident <- per.resident %>%
  filter (encountercontext == "Ambulatory")

counts <- per.resident %>%
  group_by(notetype) %>%
  summarise(
    type.time = sum(resident.time)/60/60/24,
    weekend = sum(weekend, na.rm = TRUE)/60/60/24,
    after.hours = sum(after.hours, na.rm = TRUE)/60/60/24,
    pm = sum(pm, na.rm = TRUE)/60/60/24,
    .groups = "drop"
  )

percent <- per.resident %>%
  group_by(notetype) %>%
  summarise(
    type.time = sum(resident.time)/60/60/24,
    weekend = sum(weekend, na.rm = TRUE)/60/60/24,
    after.hours = sum(after.hours, na.rm = TRUE)/60/60/24,
    pm = sum(pm, na.rm = TRUE)/60/60/24,
    .groups = "drop"
  )

  per.resident <- filtered_data %>%
    filter(pgy != "NA") %>%
    group_by(noteid, author) %>%
    summarise(
      resident.time = sum(`edittime(seconds)`),
      weekend = sum(weekend, na.rm = TRUE),
      after.hours = sum(after.hours, na.rm = TRUE),
      pm = sum(pm, na.rm = TRUE),
      across(everything(), first),
      .groups = "drop"
    )

# Summary statement
print(
    results <- c(
    paste("Total number of notes residents contributed to is:", nrow(cons.data)),
    
    paste("Atendings edited notes count is:", sum(!is.na(cons.data$`staff.time`))),
    
    paste("Others edited notes count is:", sum(!is.na(cons.data$`other.time`) & cons.data$`other.time` != 0)),
    
    paste("Total number per fiscal year, 21-22, 22-23, 23-24, respectively is:", nrow(cons.data %>% filter(dtofsvc >= ymd_hms("2021-07-01 00:00:00") & dtofsvc <= ymd_hms("2022-06-30 23:59:59"))), nrow(cons.data %>% filter(dtofsvc >= ymd_hms("2022-07-01 00:00:00") & dtofsvc <= ymd_hms("2023-06-30 23:59:59"))), nrow(cons.data %>% filter(dtofsvc >= ymd_hms("2023-07-01 00:00:00") & dtofsvc <= ymd_hms("2024-06-30 23:59:59")))),
    
    paste("Total number of residents", sum(!is.na(res_vector))),
    
    paste("Number of residents per class, PGY-3, PGY-2, and PGY-1, respectively:", resident_counts$PGY.5 + resident_counts$PGY.4 + resident_counts$PGY.3, resident_counts$PGY.4 + resident_counts$PGY.2 + resident_counts$PGY.3, resident_counts$PGY.1 + resident_counts$PGY.2 + resident_counts$PGY.3), 
    
    paste("Number of residents per FY, 21-22, 22-23, and 23-24, respectively:", resident_counts$PGY.5 + resident_counts$PGY.4 + resident_counts$PGY.3, resident_counts$PGY.4 + resident_counts$PGY.2 + resident_counts$PGY.3, resident_counts$PGY.1 + resident_counts$PGY.2 + resident_counts$PGY.3),
    
    paste("Time (days) spent on note documentation (days), total, residents, staff, and other, respectively:", sum(cons.data$total.time)/60/60/24, sum(cons.data$resident.time)/60/60/24, sum(cons.data$staff.time, na.rm = TRUE)/60/60/24, sum(cons.data$total.time)/60/60/24 - sum(cons.data$resident.time)/60/60/24 - sum(cons.data$staff.time, na.rm = TRUE)/60/60/24),
    
    paste("Time (days) spent on documentation in PM shift for inpatient notes, after-hours for ambulatory notes, and on a weekend day, respectively:", sum(cons.data$pm, na.rm = TRUE)/60/60/24, sum(cons.data$after.hours, na.rm = TRUE)/60/60/24, sum(cons.data$weekend, na.rm = TRUE)/60/60/24),

    paste("Time (days) spent on documentation for inpatient notes, and for ambulatory notes, respectively:", sum(inpatient$total.time, na.rm = TRUE)/60/60/24, sum(ambulatory$total.time, na.rm = TRUE)/60/60/24),    
    
    paste("Resident Time (days) spent on documentation in PM shift for inpatient notes, after-hours for ambulatory notes, and on a weekend day, respectively:", sum(per.resident$pm, na.rm = TRUE)/60/60/24, sum(per.resident$after.hours, na.rm = TRUE)/60/60/24, sum(per.resident$weekend, na.rm = TRUE)/60/60/24),

    paste("Resident Time (days) spent on documentation for inpatient notes, and for ambulatory notes, respectively:", sum(inpatient.resident$resident.time, na.rm = TRUE)/60/60/24, sum(ambulatory.resident$resident.time, na.rm = TRUE)/60/60/24),
    
    paste("Percentage of notes to which residents contributed >90%, >50%, and <10% of the time, respectively, is:", nrow(cons.data %>% filter(resident.percent >=90)) / nrow(cons.data) * 100, nrow(cons.data %>% filter(resident.percent >=50)) / nrow(cons.data) * 100, nrow(cons.data %>% filter(resident.percent <10)) / nrow(cons.data) * 100),
    
    paste("Number of days residents spent on each note type:", )
    
  )
)




```


```{r, counting resident note days}

all.data <- readRDS("all_data.rds")

pgy1 <- all.data %>%
  filter(pgy %in% c("pgy1.21.22", "pgy1.22.23", "pgy1.23.24"))

pgy2 <- all.data %>%
  filter(pgy %in% c("pgy2.21.22", "pgy2.22.23", "pgy2.23.24"))

pgy3 <- all.data %>%
  filter(pgy %in% c("pgy3.21.22", "pgy3.22.23", "pgy3.23.24"))
  
list.residents.1 <- split(pgy1, pgy1$author)
list.residents.2 <- split(pgy2, pgy2$author)
list.residents.3 <- split(pgy3, pgy3$author)

# Function to count unique days, considering day and night shifts (a day is between 0700 of a given day and 0700 the next day)
note_days <- function(df) {
  # Create a shift indicator based on the time
  df <- df %>%
    mutate(
      dtofsvc_date = as.Date(dtofsvc),
      shift = if_else(hour(dtofsvc) >= 7 & hour(dtofsvc) < 18, "Day", "Night"),
      # Adjust date for night shift; if it starts before midnight, it's part of the next day
      dtofsvc_date_adjusted = if_else(shift == "Night" & hour(dtofsvc) < 6, dtofsvc_date - days(1), dtofsvc_date)
    ) %>%
    summarize(count = n_distinct(dtofsvc_date_adjusted),
              time.in.days = sum(`edittime(seconds)`/60/60/24))
  
  # Return a tibble with the counts
  tibble(df)
}

# Apply this function to each dataframe in a list
list_shift_counts.1 <- lapply(list.residents.1, note_days)
list_shift_counts.2 <- lapply(list.residents.2, note_days)
list_shift_counts.3 <- lapply(list.residents.3, note_days)

# Optionally, combine all results into one dataframe with names if the list has names
combined_results <- bind_rows(list_shift_counts.1, list_shift_counts.2, list_shift_counts.3, .id = "ResidentID")

```

