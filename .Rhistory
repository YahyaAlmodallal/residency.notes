noteauthor == "botos, madison" ~ "limke, wyatt l",
TRUE ~ noteauthor
))
# Now join the fixed filtered data and the unfiltered data
all_data_5 <- bind_rows(filtered_data, unfiltered)
nrow(all_data_5) == nrow(all_data_4)
# Check: Filter all rows where a unique note's signing author is not present in the authors' column; make sure rows are 0 now
any_issues <- all_data_5 %>%
group_by(noteid) %>%
filter(!noteauthor %in% author) %>%
ungroup()
nrow(any_issues) == 0
# Cleaning resident names, adjust specific residents with changed names to match those in 'resident.names'
# Create a mapping vector (those names are inspired from previous work in 2022, however, from exploring the current data, it seems that the only different name is that of springman, alexandra which is now keating, alexandra)
name_mapping <- c(
"abu hamad, moh" = "abu hamad, moh'd rawhi abdullah m r",
"mcintire, natalie" = "mcintire, natalie r",
"daniels, elizabeth c" = "smet, elizabeth c",
"stover, daniel" = "stover, daniel w",
"ma, melinda a" = "whitacre, melinda a",
"keating, alexandra l" = "springman, alexandra l",
"cornelius, kacie" = "rytlewski, kacie",
"rytlewski, kacie t" = "rytlewski, kacie"
)
# Apply the mapping to the 'noteauthor' and 'author' columns
all_data_6 <- all_data_5 %>%
mutate(author = case_when(
author %in% names(name_mapping) ~ name_mapping[author],
TRUE ~ author
)) %>%
mutate(noteauthor = case_when(
noteauthor %in% names(name_mapping) ~ name_mapping[noteauthor],
TRUE ~ noteauthor
))
# Cleaning author service and author type (this is based on work done in 9/2023 and extended here to include any new providers; the unique noteauthors were exported, and the author service, and type, adjusted manually to crate the true.author.service file)
# Join the file with the manually adjusted data to the main file
all_data_7 <- all_data_6 %>%
left_join(true.service, by = c("noteauthor" = "name")) %>%
# Update authorsservice
mutate(authorsservice = coalesce(authorsservice.y, authorsservice.x)) %>%
# Update authortype
mutate(authortype = coalesce(authortype.y, authortype.x)) %>%
# Remove the temporary .x and .y columns for both fields
select(-matches("\\.x$"), -matches("\\.y$")) %>%
# Update the authorservice to PED General if a reisdent wrote the note and the author service is NA
mutate(authorsservice = if_else(is.na(authorsservice) & authortype == "resident", "PED General", authorsservice))
# Check that all "attendings" are present in true.service
# Step 1: Filter attending authors in all_data_7
attending_authors <- all_data_7 %>%
filter(authortype == "attending") %>%
select(noteauthor) %>%
distinct()
# Step 2: Check against true.service names
unmatched_authors <- anti_join(attending_authors, true.service, by = c("noteauthor" = "name"))
# Step 3: Provide a warning if there are discrepancies
if(nrow(unmatched_authors) > 0) {
warning("Fix the author service for any new note authors")
print(unmatched_authors) # Optionally print out the unmatched authors for review
} else {
print("All attending note authors are correctly matched in true.service")
}
# Step 4: Provide a warning if there are discrepancies
if(nrow(all_data_3) != nrow(all_data_7)) {
warning("You have lost some rows while refining the data")
print(unmatched_authors) # Optionally print out the unmatched authors for review
} else {
print("You have not lost any rows while refining the data")
}
# Fixing the PED Nursery author service
all_data_8 <- all_data_7 %>%
mutate(
# Create a new column 'NNSY' based on specified conditions
nnsy = ifelse(
abs(difftime(dtofsvc, birthdate, units = "days")) <= 3 &
authorsservice %in% c("PED General", "PED Nursery", "Pediatrics") &
encountercontext %in% c("Inpatient") &
notetype != "Clinic Note",
"Yes",
"No"
)
) %>%
# Update 'Author.s.Service' for 'NNSY' = "Yes"
mutate(
authorsservice = ifelse(nnsy == "Yes", "PED Nursery", authorsservice),
) %>%
# Remove unneeded columns
select(-nnsy)
summary.6 = sum_df(all_data_8)
# Determine PGY for each resident and for each time interval
# Define the date ranges
date_21_start <- as.POSIXct("2021-07-01 00:00:00")
date_21_end <- as.POSIXct("2022-06-30 23:59:59")
date_22_start <- as.POSIXct("2022-07-01 00:00:00")
date_22_end <- as.POSIXct("2023-06-30 23:59:59")
date_23_start <- as.POSIXct("2023-07-01 00:00:00")
date_23_end <- as.POSIXct("2024-06-30 23:59:59")
before_end <- as.POSIXct("2021-06-30 23:59:59")
after_start <- as.POSIXct("2024-07-01 00:00:00")
# Filter the data based on the date ranges
data_21 <- all_data_8 %>% filter(editstart >= date_21_start & editstart <= date_21_end)
data_22 <- all_data_8 %>% filter(editstart >= date_22_start & editstart <= date_22_end)
data_23 <- all_data_8 %>% filter(editstart >= date_23_start & editstart <= date_23_end)
before <- all_data_8 %>% filter(editstart <= before_end)
after <- all_data_8 %>% filter(editstart >= after_start)
# Check that no values were missed
total_rows <- nrow(all_data_8)
filtered_rows <- nrow(data_21) + nrow(data_22) + nrow(data_23) + nrow(before) + nrow(after)
if(total_rows == filtered_rows) {
print("All rows have been accounted for.")
} else {
print("Some rows have been missed.")
}
# Apply conditions directly without a function
data_21 <- data_21 %>%
mutate(pgy = case_when(
author %in% resident.names$`PGY.3` ~ "pgy1.21.22",
author %in% resident.names$`PGY.4` ~ "pgy2.21.22",
author %in% resident.names$`PGY.5` ~ "pgy3.21.22",
TRUE ~ NA_character_
))
# Apply conditions directly without a function
data_22 <- data_22 %>%
mutate(pgy = case_when(
author %in% resident.names$`PGY.2` ~ "pgy1.22.23",
author %in% resident.names$`PGY.3` ~ "pgy2.22.23",
author %in% resident.names$`PGY.4` ~ "pgy3.22.23",
TRUE ~ NA_character_
))
# Apply conditions directly without a function
data_23 <- data_23 %>%
mutate(pgy = case_when(
author %in% resident.names$`PGY.1` ~ "pgy1.23.24",
author %in% resident.names$`PGY.2` ~ "pgy2.23.24",
author %in% resident.names$`PGY.3` ~ "pgy3.23.24",
TRUE ~ NA_character_
))
# join the 3 dataframes, leaving out those outside the date range
all_data_8 <- bind_rows(data_21, data_22, data_23)
# Remove these rows from the original data frame
all_data_9 <- all_data_8 %>%
filter(
(`edittime(seconds)` > 5 & !author %in% staffs) |  # Handle edittime and author conditions
(`edittime(seconds)` > 0 & author %in% staffs)   # Handle edittime and author conditions
)
all_data_9 <- all_data_9 %>%
filter((is.na(notesize) | notesize != 0))    # Keep notesize that are NA or not zero
nrow(all_data_8)
nrow(all_data_9)
# These are unaccounted for since they are overlapped between the criteria above (e.g. <5 seconds and before 6/30/21)
removed = - nrow(all_data_9) + nrow(all_data_8)
print(removed)
summary.7 <- sum_df(all_data_9)
# Delete all notes where a resident did not have a true contribution (<5 seconds). We will group the dataframe by note id, then look at all the editing instances of each unique note id, and if the pgy column do not include one of the residents (all its values are), then will delete these note ids
filtered_data <- all_data_9 %>%
group_by(noteid) %>%
filter(!all(is.na(pgy))) %>%
ungroup()
cons.data <- filtered_data %>%
group_by(noteid) %>%
group_by(noteid) %>%
summarise(total.time = sum(`edittime(seconds)`),
across(everything(), first),
.groups = "drop")
# 2- attendings file (consolidated by ID, all attending edits for each note combined), delete unneeded columns
staff.data <- filtered_data %>%
filter(author %in% staffs) %>%
group_by(noteid) %>%
summarise(staff.time = sum(`edittime(seconds)`),
across(everything(), first),
.groups = "drop")
# 3- resident and class files (consolidated by ID and class/resident, all notes), delete unneeded columns
resident.data <- filtered_data %>%
filter(pgy != "NA") %>%
group_by(noteid) %>%
summarise(resident.time = sum(`edittime(seconds)`),
across(everything(), first),
.groups = "drop")
# Join the resident and staff eidting time columns to the main dataframe
# Join staff.time from staff.data
cons.data <- cons.data %>%
left_join(staff.data %>% select(noteid, staff.time), by = "noteid")
# Join resident.time from resident.data
cons.data <- cons.data %>%
left_join(resident.data %>% select(noteid, resident.time), by = "noteid")
# Summary statement
summary_messages <- c(
paste("At the beginning, there were", nrow(all_data_1), "notes. Out of these, notes with equal or incomplete edit time data counted", nrow(not_equal), "and", nrow(no_times), ", respectively.", na_count, "notes had incorrect times to calculate a start-to-stop value. The file was then separated for each edit instance resulting in", nrow(all_data_3), "instances, of which", nrow(all_data_8) - nrow(all_data_9), "were removed because these lasted <5 seconds and were not attending edits or lasted <1 second and were attending edits, or contained 0 characters.", nrow(before) + nrow(after), "editing instances were removed because they occurred before 7/1/21 or after 6/30/24. Further", nrow(all_data_9) - nrow(filtered_data), "editing instances were removed since there were no true resident contributions to the associated notes. The final note count after refinements is", nrow(cons.data), "meaning that", (nrow(all_data_2) - nrow(cons.data)), "notes were excluded while editing instances were refined."),
paste("Total number of excluded notes is (both numbers should be equal):", nrow(not_equal) + nrow(no_times) + (nrow(all_data_2) - nrow(cons.data)), nrow(all_data_1) - nrow(cons.data))
)
# To print the messages, you can then do:
print(summary_messages[1])  # Print first message
print(summary_messages[2])  # Print second message
saveRDS(filtered_data, file = "all_data.rds")
saveRDS(cons.data, file = "cons_data.rds")
saveRDS(staff.data, file = "staff_data.rds")
saveRDS(resident.data, file = "resident_data.rds")
saveRDS(summary_messages, file = "summary_messages.rds")
#Clean environment and free memory
rm(list = ls())
gc()
setwd("C:\\Users\\yahya\\OneDrive\\Research\\residency.notes\\Script")
save_path <- "C:\\Users\\yahya\\OneDrive\\Research\\residency.notes\\Script\\Script cache and figures\\Cache"
knitr::opts_chunk$set(
echo = FALSE,
warning = FALSE,
message = FALSE,
fig.width = 10,
fig.height = 8,
cache = TRUE,
cache.path = save_path,
fig.path = save_path
)
#Set number of digits to display
options(digits = 5, scipen = 999)
#Packages and Libraries
library(ggplot2)
library(tidyverse)
library(lubridate)
library(readr)
library(conflicted)
library(dplyr)
library(htmltools)
library(skimr)
library(janitor)
library(gt)
library(tidyr)
library(stringdist)
library(purrr)
library(report)
library(grid)
library(stringr)
library(gridExtra)
library(MASS)
library(knitr)
library(tools)
library(kableExtra)
library(DT)
library(broom)
library(htmlwidgets)
library(webshot)
library(extrafont)
library(ragg)
library(plotly)
library(pagedown)
library(readxl)
library(rlang)
library(nortest)
conflict_prefer("select", "dplyr")
conflicts_prefer(dplyr::filter)  # Set conflict resolution preferences
#summarize dataframes
sum_df <- function(df) {
# Create an empty list to store summaries
summary_list <- list()
# 1. Number of rows
summary_list$Number_of_Rows <- nrow(df)
# 2. Names of columns
summary_list$Column_Names <- colnames(df)
# 3. Summaries for character variables with fewer than 100 unique values
char_vars <- df[, sapply(df, is.character)]
if (!is.null(char_vars)) {
summary_list$Character_Variables <- lapply(char_vars, function(x) {
if (length(unique(x)) < 100) {
return(table(x))
} else {
return(NA)  # Or return(NULL) if you prefer not to store anything for columns with >= 100 unique values
}
})
}
# 4. Summaries for numerical variables
num_vars <- df[, sapply(df, is.numeric)]
if (!is.null(num_vars)) {
summary_list$Numerical_Variables <- lapply(num_vars, function(x) {
list(
Median = median(x, na.rm = TRUE),
IQR = IQR(x, na.rm = TRUE),
Min = min(x, na.rm = TRUE),
Max = max(x, na.rm = TRUE)
)
})
}
# 5. First and last dates for POSIXct variables
date_vars <- df[, sapply(df, function(x) inherits(x, "POSIXct"))]
if (!is.null(date_vars)) {
summary_list$Date_Variables <- lapply(date_vars, function(x) {
list(
First_Date = min(x, na.rm = TRUE),
Last_Date = max(x, na.rm = TRUE)
)
})
}
return(summary_list)
}
# Determine the 'Shift'
get_shift <- function(encounter, editstart) {
hour <- hour(editstart)
if (encounter %in% c("Ambulatory", "HOV", "Telephone Encounter")) {
if (hour >= 7 & hour < 17) {
return("Work-hours")
} else {
return("After work-hours")
}
} else if (encounter == "Emergency Department") {
return(NA)
} else if (encounter %in% "Inpatient") {
if (hour >= 7 & hour < 18) {
return("AM shift")
} else {
return("PM shift")
}
} else {
return(NA)
}
}
# Function to create and save plots for character variables in EDA
create_and_save_plots <- function(data, columns, file_name_prefix, ncol = 3) {
plots <- lapply(columns, function(column) {
ggplot(data, aes(x = !!sym(column))) +
geom_bar() +
geom_text(stat='count', aes(label=..count..), vjust=-0.5) +
theme(axis.text.x = element_text(angle = 90, hjust = 1))
})
combined_plot <- do.call(grid.arrange, c(plots, ncol = ncol))
# Save the combined figure
ggsave(paste0(file_name_prefix, "_combined_plot.png"), plot = combined_plot, width = 20, height = 15)
}
# Function to create and save plots for numeric variables in EDA
create_and_save_numeric_plots <- function(data, file_name_prefix, ncol = 3) {
numeric_cols <- data %>%
select_if(is.numeric) %>%
colnames()
plots <- lapply(numeric_cols, function(column) {
hist_plot <- ggplot(data, aes(x = !!sym(column))) +
geom_histogram(bins = 60) +
labs(title = column)
list(hist_plot)
})
plots <- unlist(plots, recursive = FALSE)
# Split plots into groups of 6 and save each group
plot_groups <- split(plots, ceiling(seq_along(plots)/6))
for (i in seq_along(plot_groups)) {
combined_plot <- do.call(grid.arrange, c(plot_groups[[i]], ncol = ncol))
ggsave(paste0(file_name_prefix, "_combined_plot", i, ".png"), plot = combined_plot, width = 20, height = 15)
}
}
# Function to apply transformations, sample means, create plots, and perform tests
analyze_transformations <- function(data, columns, seed = 12) {
test_results <- data.frame(
variable = character(),
transformation = character(),
ad_p_value = numeric(),
shapiro_p_value = numeric(),
stringsAsFactors = FALSE
)
transformations <- list(
original = function(x) x,
log = function(x) ifelse(x > 0, log(x), NA),
sqrt = function(x) ifelse(x >= 0, sqrt(x), NA),
cbrt = function(x) ifelse(x >= 0, x^(1/3), NA),
boxcox = function(x) {
x_positive <- x + 1
lambda <- boxcox(x_positive ~ 1, lambda = seq(-2, 2, by = 0.05))$x[which.max(boxcox(x_positive ~ 1, lambda = seq(-2, 2, by = 0.05))$y)]
(x_positive)^lambda
}
)
for (column in columns) {
for (trans_name in names(transformations)) {
# Apply transformation
trans_data <- data %>% mutate(trans_col = transformations[[trans_name]](!!sym(column)))
# Remove NA values from the transformed data
clean_data <- na.omit(trans_data$trans_col)
if (length(clean_data) >= 500) {  # Ensure there are enough data points for sampling
set.seed(seed)  # for reproducibility
sample_means <- replicate(1000, mean(sample(clean_data, size = 500, replace = TRUE)))
# Create histogram
hist(sample_means, breaks = 60, main = paste("Histogram of Sample Means -", column, trans_name))
# Create QQ plot
qqnorm(sample_means, main = paste("QQ Plot of Sample Means -", column, trans_name))
qqline(sample_means, col = "red")
# Perform tests
ad_test <- ad.test(sample_means)
shapiro_test <- shapiro.test(sample_means)
# Store results
test_results <- rbind(test_results, data.frame(
variable = column,
transformation = trans_name,
ad_p_value = ad_test$p.value,
shapiro_p_value = shapiro_test$p.value
))
} else {
message(paste("Not enough valid data points for", column, "with transformation", trans_name))
}
}
}
return(test_results)
}
# read the main file
cons.data <- readRDS("cons_data.rds")
k = sum_df(cons.data)
# Columns to be deleted
cols_to_delete <- c("birthdate", "author", "day", "editstart", "editstop", "edittime(seconds)",
"noteauthorlogindept", "noteauthornpi", "noteduration(physician)",
"noteduration(resident)", "shift", "specialtyatnote",
"specialtycurrent", "timediff(hr)")
# Delete specified columns
cons.data <- cons.data %>% select(-all_of(cols_to_delete))
# Create a new column "other.time"
cons.data <- cons.data %>%
mutate(other.time = total.time - staff.time - resident.time)
# Change "smartphraseids" into a numerical column by counting the number of occurrences of "["
cons.data <- cons.data %>%
mutate(smartphraseids = str_count(smartphraseids, "\\["))
# Define the columns to be plotted
columns1 <- c("notetype", "cosigninformation", "notestatus", "authorsservice", "encountercontext", "sharedwithpatient")
columns2 <- c("reasonforblockingnote", "ccas", "pgy")
# Create and save plots
create_and_save_plots(cons.data, columns1, "combined_plot1")
create_and_save_plots(cons.data, columns2, "combined_plot2")
# Create and save plots for numeric variables
create_and_save_numeric_plots(cons.data, "numeric_plot")
#Given the very large number of measurements in the dataset, the data, untransformed, vs the data after several different transformations was tested as to whether it meets the central limit theorem (CLT, i.e. 1000 samples, each of 500 measurements - reflecting the large subsets we would work with - were taken and the means were QQ plotted and tested for normality using the Anderson-Darling normality and Shapiro-Walk normality tests). This was done to the 9 numeric variables below and the log transformation, with its ease of use, was thought to be the best transformation to use as all the transformed variables met normality estimate and therefore the CLT was assumed to apply for each.
# Define the columns to be analyzed
columns <- c("total.time", "resident.time", "staff.time", "other.time", "notesize", "%copied", "%voice", "starttostop", "smartphraseids")
# Perform analysis
results <- analyze_transformations(cons.data, columns)
# Filter and summarize data for each note type
plot_data <- cons.data %>%
filter(notetype %in% c("Clinic Note", "H&P", "Progress Note", "Discharge Summary")) %>%
group_by(month = floor_date(dtofsvc, "month"), notetype) %>%
summarise(mean_time = mean(total.time), .groups = 'drop')
# Plotting based on dtofsvc
plot_dtofsvc <- ggplot(plot_data, aes(x = month, y = mean_time, color = notetype)) +
geom_line() +
labs(title = "Monthly Trend of Total Time by Note Type (dtofsvc)",
x = "Month",
y = "Average Total Time (min)",
color = "Note Type") +
theme_minimal()
# Save the plot to working directory
ggsave("plot_dtofsvc.png", plot = plot_dtofsvc, width = 10, height = 6, dpi = 300)
# Adjust data filtering and summarization for filetime
plot_data_filetime <- cons.data %>%
filter(notetype %in% c("Clinic Note", "H&P", "Progress Note", "Discharge Summary")) %>%
group_by(month = floor_date(filetime, "month"), notetype) %>%
summarise(mean_time = mean(total.time), .groups = 'drop')
# Plotting based on filetime
plot_filetime <- ggplot(plot_data_filetime, aes(x = month, y = mean_time, color = notetype)) +
geom_line() +
labs(title = "Monthly Trend of Total Time by Note Type (filetime)",
x = "Month",
y = "Average Total Time (min)",
color = "Note Type") +
theme_minimal()
# Save the plot to working directory
ggsave("plot_filetime.png", plot = plot_filetime, width = 10, height = 6, dpi = 300)
print(results)
summary <- readRDS("summary_messages.rds")
print(summary)
all.data <- readRDS("all_data.rds")
View(all.data)
str(all.data)
#Clean environment and free memory
rm(list = ls())
gc()
setwd("C:\\Users\\yahya\\OneDrive\\Research\\residency.notes\\Script")
save_path <- "C:\\Users\\yahya\\OneDrive\\Research\\residency.notes\\Script\\Script cache and figures\\Cache"
knitr::opts_chunk$set(
echo = FALSE,
warning = FALSE,
message = FALSE,
fig.width = 10,
fig.height = 8,
cache = TRUE,
cache.path = save_path,
fig.path = save_path
)
#Set number of digits to display
options(digits = 5, scipen = 999)
#Packages and Libraries
library(ggplot2)
library(tidyverse)
library(lubridate)
library(readr)
library(conflicted)
library(dplyr)
library(htmltools)
library(skimr)
library(janitor)
library(gt)
library(tidyr)
library(stringdist)
library(purrr)
library(report)
library(grid)
library(stringr)
library(gridExtra)
library(MASS)
library(knitr)
library(tools)
library(kableExtra)
library(DT)
library(broom)
library(htmlwidgets)
library(webshot)
library(extrafont)
library(ragg)
library(plotly)
library(pagedown)
library(readxl)
library(rlang)
library(nortest)
conflict_prefer("select", "dplyr")
conflicts_prefer(dplyr::filter)  # Set conflict resolution preferences
